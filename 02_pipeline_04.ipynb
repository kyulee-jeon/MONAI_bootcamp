{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "#  MONAI Bootcamp\n",
    "## End-To-End Workflow with MONAI part 4 ( data partition + Ignite Supervised Evaluator and Trainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_lPbckSZsAG"
   },
   "source": [
    "# same baseline End-to-end Training with Ignite\n",
    "We've covered a lot of material and now it's time to apply the things that we've learned in an end-to-end example. First, we're going to use the basic PyTorch paradigm for training our model. We'll then look at how to train using the Ignite workflows to make things even easier!\n",
    "\n",
    "## baseline  End-to-End Training Workflow\n",
    "To help guide you through training your first model using MONAI, this guide will will cover five key phases:\n",
    "\n",
    " 1. Setting up our Dataset and exploring the data\n",
    " 2. Preparing datasets and transforms\n",
    " 3. Define your network and create our PyTorch training loop [replace with ignite]\n",
    " 4. Evaluate your model and understand the results\n",
    " \n",
    "Let's get started by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qg9upTKtVga-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import monai\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.data import decollate_batch, partition_dataset_classes\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    EnsureType\n",
    ")\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check GPU Memory with `nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  5 03:03:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                   On |\n",
      "| N/A   32C    P0    62W / 400W |   4682MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  0    5   0   0  |   4656MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\n",
      "|                  |      2MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0    5    0    3206127      C   /opt/conda/bin/python3.8         4637MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBZIJLrJZ1IQ"
   },
   "source": [
    "## 1. Setting up our Dataset and exploring the data\n",
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called temp directory in `~/monai-lab/temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOX-pVqQVo01",
    "outputId": "cc1d2154-c38c-459b-92c9-4b9263447b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "directory = \"temp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQQ26Fv9Z-xK"
   },
   "source": [
    "Download the MedNIST dataset\n",
    "The MedNIST dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions), [the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4), and the [NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest).\n",
    "\n",
    "The dataset is kindly made available by [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license. If you use the MedNIST dataset, please acknowledge the source.\n",
    "\n",
    "We're going to download this dataset below and extract it into our temporary MONAI Data Directory.\n",
    "It will take about 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_yH3E2_VvX7",
    "outputId": "04200657-6aea-4747-ad99-efe377a0dce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 570 µs, sys: 0 ns, total: 570 µs\n",
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "resource = \"https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz?dl=1\"\n",
    "md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"MedNIST.tar.gz\")\n",
    "data_dir = os.path.join(root_dir, \"MedNIST\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccI6haMfaYSF"
   },
   "source": [
    "### Set deterministic training for reproducibility\n",
    "\n",
    "[`set_determinism`](https://docs.monai.io/en/latest/utils.html?highlight=set_determinism#monai.utils.misc.set_determinism) will set the random seeds in both Numpy and PyTorch to ensure reproducibility. We'll see later that we need to go a little bit further to ensure reproducibility in a jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EwYpFBImV1hJ"
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uKHIr0Jaj2s"
   },
   "source": [
    "#### Read the image filenames from the dataset folders\n",
    "\n",
    "When using a dataset, you want to understand the basics of the images, labels, and more. We'll start off by showing some of those basic statistics for MedNIST.\n",
    "\n",
    "We'll see that 6 different folders are representing 6 different categories: Hand, AbdomenCT, CXR, ChestCT, BreastMRI, HeadCT. We'll be using each of these categories as our label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4T6KV7lV1ng",
    "outputId": "933bcb83-46c1-472d-b5db-c0e786b816c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image count: 58954\n",
      "Image dimensions: 64 x 64\n",
      "Label names: ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']\n",
      "number of Labels: 6\n",
      "Label counts: [10000, 8954, 10000, 10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "class_names = sorted(x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, x)))\n",
    "num_class = len(class_names)\n",
    "\n",
    "image_files = [\n",
    "    [\n",
    "        os.path.join(data_dir, class_names[i], x)\n",
    "        for x in os.listdir(os.path.join(data_dir, class_names[i]))\n",
    "    ]\n",
    "    for i in range(num_class)\n",
    "]\n",
    "\n",
    "num_each = [len(image_files[i]) for i in range(num_class)]\n",
    "image_files_list = []\n",
    "image_class = []\n",
    "\n",
    "for i in range(num_class):\n",
    "    image_files_list.extend(image_files[i])\n",
    "    image_class.extend([i] * num_each[i])\n",
    "    \n",
    "num_total = len(image_class)\n",
    "image_width, image_height = PIL.Image.open(image_files_list[0]).size\n",
    "\n",
    "print(f\"Total image count: {num_total}\")\n",
    "print(f\"Image dimensions: {image_width} x {image_height}\")\n",
    "print(f\"Label names: {class_names}\")\n",
    "print(f\"number of Labels: {num_class}\")\n",
    "print(f\"Label counts: {num_each}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx-WedqRWDbt"
   },
   "source": [
    "# 2. Preparing datasets and transforms\n",
    "### Prepare training, validation, and test data lists\n",
    "\n",
    "We want to split the data into 3 different sets, one for training, one for validation, and one for testing. We'll use a ratio of 80/10/10 for those sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vppWmX-BV1uN",
    "outputId": "74075068-0c62-4b37-8371-3cd0b9d0fa27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training count: 46946, Validation count: 6022, Test count: 5986\n"
     ]
    }
   ],
   "source": [
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "train_x = list()\n",
    "train_y = list()\n",
    "val_x = list()\n",
    "val_y = list()\n",
    "test_x = list()\n",
    "test_y = list()\n",
    "\n",
    "for i in range(num_total):\n",
    "    rann = np.random.random()\n",
    "    if rann < val_frac:\n",
    "        val_x.append(image_files_list[i])\n",
    "        val_y.append(image_class[i])\n",
    "    elif rann < test_frac + val_frac:\n",
    "        test_x.append(image_files_list[i])\n",
    "        test_y.append(image_class[i])\n",
    "    else:\n",
    "        train_x.append(image_files_list[i])\n",
    "        train_y.append(image_class[i])\n",
    "\n",
    "print(f\"Training count: {len(train_x)}, Validation count: {len(val_x)}, Test count: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manual with floor math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47164 5895 5895\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "# make this selection deterministic and controllable by seed\n",
    "dataset_seed = 12345678\n",
    "r = np.random.RandomState(dataset_seed)\n",
    "\n",
    "# calculate the number of images we want for the validation and test groups\n",
    "validation_proportion = 0.1\n",
    "test_proportion = 0.1\n",
    "validation_count = floor(validation_proportion * num_total)\n",
    "test_count = floor(test_proportion * num_total)\n",
    "\n",
    "groups = np.zeros(num_total, dtype=np.int32)\n",
    "\n",
    "# set the appropriate number of '1's for the validation dataset\n",
    "groups[:validation_count] = 1\n",
    "\n",
    "# then set the appropriate number of '2's for the test dataset\n",
    "groups[validation_count:validation_count + test_count] = 2\n",
    "\n",
    "# Shuffle the sequence so that \n",
    "r.shuffle(groups)\n",
    "\n",
    "image_sets = list(), list(), list()\n",
    "label_sets = list(), list(), list()\n",
    "\n",
    "for n in range(num_total):\n",
    "    image_sets[groups[n]].append(image_files_list[n])\n",
    "    label_sets[groups[n]].append(image_class[n])\n",
    "    \n",
    "train_x, val_x, test_x = image_sets\n",
    "train_y, val_y, test_y = label_sets\n",
    "print(len(train_x), len(val_x), len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition with shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training count: 47163, Validation count: 5895, Test count: 5895\n"
     ]
    }
   ],
   "source": [
    "train_inds, val_inds, test_inds = partition_dataset_classes(np.arange(len(image_files_list)), \n",
    "                                                            image_class,(8, 1, 1), shuffle=True)\n",
    "\n",
    "train_x = [image_files_list[i] for i in train_inds]\n",
    "train_y = [image_class[i] for i in train_inds]\n",
    "val_x = [image_files_list[i] for i in val_inds]\n",
    "val_y = [image_class[i] for i in val_inds]\n",
    "test_x = [image_files_list[i] for i in test_inds]\n",
    "test_y = [image_class[i] for i in test_inds]\n",
    "\n",
    "print(f\"Training count: {len(train_x)}, Validation count: {len(val_x)}, Test count: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition with suffle and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47163 5895 5895\n"
     ]
    }
   ],
   "source": [
    "rseed = 12345678\n",
    "parts = partition_dataset_classes(\n",
    "    data=np.arange(len(image_files_list)), \n",
    "    classes=image_class, \n",
    "    ratios=(8, 1, 1), \n",
    "    shuffle=True, \n",
    "    seed=rseed\n",
    ")\n",
    "\n",
    "image_sets = [list(), list(), list()]\n",
    "label_sets = [list(), list(), list()]\n",
    "\n",
    "for i, part in enumerate(parts):\n",
    "    image_sets[i] = [image_files_list[idx] for idx in part]\n",
    "    label_sets[i] = [image_class[idx] for idx in part]\n",
    "\n",
    "train_x, val_x, test_x = image_sets\n",
    "train_y, val_y, test_y = label_sets\n",
    "print(len(train_x), len(val_x), len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQKPHOjgWHoW"
   },
   "source": [
    "### Define MONAI transforms, Dataset and Dataloader to pre-process data\n",
    "\n",
    "We'll define our transform using `Compose`. In this Array of Transforms, we'll load the image, add a channel, scale its intensity, utilize a few random functions and finally create a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaRvhLd-WTmF",
    "outputId": "619dac29-2ec4-4bb3-bf52-9bf49780c17e"
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True),\n",
    "        AddChannel(),\n",
    "        ScaleIntensity(),\n",
    "        RandRotate(range_x=15, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose([LoadImage(image_only=True), AddChannel(), ScaleIntensity(), ToTensor()])\n",
    "\n",
    "act = Compose([EnsureType(), Activations(softmax=True)])\n",
    "to_onehot = Compose([EnsureType(), AsDiscrete(to_onehot=num_class, n_classes=num_class)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DeElxN0WcKt"
   },
   "source": [
    "### Initialise the datasets and loaders for training, validation and test sets\n",
    "- Define a simple dataset, that we'll call `MedNISTDataset`, that  groups:\n",
    "\n",
    " - Images\n",
    " - Labels\n",
    " - The transforms that are to be run on the images and labels\n",
    "- Create three instances of this dataset:\n",
    "  - One for training\n",
    "  - One for validation\n",
    "  - One for testing\n",
    "\n",
    "We'll use a batch size of 512 and employ 10 workers to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fag4Yd2CWTum",
    "outputId": "af0aca02-0916-4909-9da7-8a14f57c1a20"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 10\n",
    "\n",
    "class MedNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_files, labels, transforms):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]\n",
    "\n",
    "\n",
    "train_ds = MedNISTDataset(train_x, train_y, train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_ds = MedNISTDataset(val_x, val_y, val_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "test_ds = MedNISTDataset(test_x, test_y, val_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df90U_QrW0i6"
   },
   "source": [
    "3. Define your network and create our PyTorch training loop\n",
    "Define network and optimizer\n",
    "Set learning_rate for how much the model is updated per step\n",
    "The fetch a pytorch device for the GPU\n",
    "Instantiate a `densenet121` model instance and 'send' it to the GPU using device\n",
    "This is a standard MONAI implementation; it is capable of 2D and 3D operation but here we are using it in 2D mode\n",
    "We'll make use of the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B0Fxk7CNWTxg"
   },
   "outputs": [],
   "source": [
    "# Configure \n",
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = DenseNet121(spatial_dims=2, in_channels=1, out_channels=num_class).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyIfXHW9W3rW"
   },
   "source": [
    "\n",
    "## Let's use ignite (Supervised Evaluator and trainer)\n",
    "Everything that we have done so far uses MONAI with pytorch in a very vanilla fashion. The initial training / validation loop is written to show you the nuts and bolts of pytorch. Now let's explore starting the move towards Ignite and features of MONAI designed to work with it.\n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/workflows.png\" width=600>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize network, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure \n",
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = DenseNet121(spatial_dims=2, in_channels=1, out_channels=num_class).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load ignite module and initialize  buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy\n",
    "from monai.handlers import ROCAUC, ValidationHandler\n",
    "from monai.engines import SupervisedTrainer, SupervisedEvaluator\n",
    "step = 1\n",
    "train_epochs = 4\n",
    "iter_losses = []\n",
    "batch_sizes = []\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure roc metric (same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "steps_per_epoch = len(train_ds) // train_loader.batch_size\n",
    "if len(train_ds) % train_loader.batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "\n",
    "def roc_auc_trans(x):\n",
    "    if isinstance(x, list):\n",
    "        pred = torch.cat([i[0][None, :] for i in x])\n",
    "        label = torch.cat([i[1][None, :] for i in x])\n",
    "        return pred, label\n",
    "\n",
    "    return act(x[\"pred\"]), to_onehot(x[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure ignite Supervised Evaluator and trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_batch(batchdata, device, non_blocking):\n",
    "    img, classes = batchdata\n",
    "    return img.to(device), classes.to(device)\n",
    "\n",
    "\n",
    "evaluator = SupervisedEvaluator(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    postprocessing=roc_auc_trans,\n",
    "    key_val_metric={\"rocauc\": ROCAUC(output_transform=roc_auc_trans)},\n",
    "    prepare_batch=prepare_batch,\n",
    ")\n",
    "\n",
    "trainer = SupervisedTrainer(\n",
    "    device=device,\n",
    "    max_epochs=train_epochs,\n",
    "    train_data_loader=train_loader,\n",
    "    network=net,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    train_handlers=[ValidationHandler(1, evaluator)],\n",
    "    prepare_batch=prepare_batch,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure event handler for iteration and epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IBDD_Iq0Yh5j"
   },
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def _end_iter(engine):\n",
    "    global step\n",
    "    loss = np.average([o[\"loss\"] for o in engine.state.output])\n",
    "    batch_len = len(engine.state.batch[0])\n",
    "    epoch = engine.state.epoch\n",
    "    epoch_len = engine.state.max_epochs\n",
    "    step_total = engine.state.iteration  \n",
    "    iter_losses.append(loss)\n",
    "    batch_sizes.append(batch_len)\n",
    "\n",
    "    print(f\"epoch {epoch}/{epoch_len}, step {step}/{steps_per_epoch}, total step {step_total}/{steps_per_epoch*epoch_len}, training_loss = {loss:.4f}\")\n",
    "    step += 1\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def run_validation(engine):\n",
    "    global step\n",
    "    # the overall average loss must be weighted by batch size\n",
    "    overall_average_loss = np.average(iter_losses, weights=batch_sizes)\n",
    "    epoch_loss_values.append(overall_average_loss)\n",
    "\n",
    "    # clear the contents of iter_losses and batch_sizes for the next epoch\n",
    "    del iter_losses[:]\n",
    "    del batch_sizes[:]\n",
    "\n",
    "    # fetch and report the validation metrics\n",
    "    roc = evaluator.state.metrics[\"rocauc\"]\n",
    "    metric_values.append(roc)\n",
    "    print(f\"evaluation for epoch {engine.state.epoch},  rocauc = {roc:.4f}\")\n",
    "    step = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### launch ignite trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTtDjwG3YkJZ",
    "outputId": "ce99b12f-234f-459e-f7db-4ad30d3b86f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/4, step 1/185, total step 1/740, training_loss = 1.8379\n",
      "epoch 1/4, step 2/185, total step 2/740, training_loss = 1.8128\n",
      "epoch 1/4, step 3/185, total step 3/740, training_loss = 1.7925\n",
      "epoch 1/4, step 4/185, total step 4/740, training_loss = 1.7243\n",
      "epoch 1/4, step 5/185, total step 5/740, training_loss = 1.7312\n",
      "epoch 1/4, step 6/185, total step 6/740, training_loss = 1.7286\n",
      "epoch 1/4, step 7/185, total step 7/740, training_loss = 1.7323\n",
      "epoch 1/4, step 8/185, total step 8/740, training_loss = 1.6924\n",
      "epoch 1/4, step 9/185, total step 9/740, training_loss = 1.6641\n",
      "epoch 1/4, step 10/185, total step 10/740, training_loss = 1.6230\n",
      "epoch 1/4, step 11/185, total step 11/740, training_loss = 1.6281\n",
      "epoch 1/4, step 12/185, total step 12/740, training_loss = 1.5930\n",
      "epoch 1/4, step 13/185, total step 13/740, training_loss = 1.5854\n",
      "epoch 1/4, step 14/185, total step 14/740, training_loss = 1.5791\n",
      "epoch 1/4, step 15/185, total step 15/740, training_loss = 1.5381\n",
      "epoch 1/4, step 16/185, total step 16/740, training_loss = 1.4975\n",
      "epoch 1/4, step 17/185, total step 17/740, training_loss = 1.4709\n",
      "epoch 1/4, step 18/185, total step 18/740, training_loss = 1.4743\n",
      "epoch 1/4, step 19/185, total step 19/740, training_loss = 1.4457\n",
      "epoch 1/4, step 20/185, total step 20/740, training_loss = 1.4533\n",
      "epoch 1/4, step 21/185, total step 21/740, training_loss = 1.4335\n",
      "epoch 1/4, step 22/185, total step 22/740, training_loss = 1.4101\n",
      "epoch 1/4, step 23/185, total step 23/740, training_loss = 1.3758\n",
      "epoch 1/4, step 24/185, total step 24/740, training_loss = 1.3760\n",
      "epoch 1/4, step 25/185, total step 25/740, training_loss = 1.3849\n",
      "epoch 1/4, step 26/185, total step 26/740, training_loss = 1.3531\n",
      "epoch 1/4, step 27/185, total step 27/740, training_loss = 1.3037\n",
      "epoch 1/4, step 28/185, total step 28/740, training_loss = 1.3538\n",
      "epoch 1/4, step 29/185, total step 29/740, training_loss = 1.2780\n",
      "epoch 1/4, step 30/185, total step 30/740, training_loss = 1.2753\n",
      "epoch 1/4, step 31/185, total step 31/740, training_loss = 1.2417\n",
      "epoch 1/4, step 32/185, total step 32/740, training_loss = 1.2146\n",
      "epoch 1/4, step 33/185, total step 33/740, training_loss = 1.2484\n",
      "epoch 1/4, step 34/185, total step 34/740, training_loss = 1.2078\n",
      "epoch 1/4, step 35/185, total step 35/740, training_loss = 1.1994\n",
      "epoch 1/4, step 36/185, total step 36/740, training_loss = 1.1440\n",
      "epoch 1/4, step 37/185, total step 37/740, training_loss = 1.1402\n",
      "epoch 1/4, step 38/185, total step 38/740, training_loss = 1.1449\n",
      "epoch 1/4, step 39/185, total step 39/740, training_loss = 1.1584\n",
      "epoch 1/4, step 40/185, total step 40/740, training_loss = 1.1657\n",
      "epoch 1/4, step 41/185, total step 41/740, training_loss = 1.1341\n",
      "epoch 1/4, step 42/185, total step 42/740, training_loss = 1.0850\n",
      "epoch 1/4, step 43/185, total step 43/740, training_loss = 1.1025\n",
      "epoch 1/4, step 44/185, total step 44/740, training_loss = 1.1045\n",
      "epoch 1/4, step 45/185, total step 45/740, training_loss = 1.0934\n",
      "epoch 1/4, step 46/185, total step 46/740, training_loss = 1.0246\n",
      "epoch 1/4, step 47/185, total step 47/740, training_loss = 1.0728\n",
      "epoch 1/4, step 48/185, total step 48/740, training_loss = 1.0660\n",
      "epoch 1/4, step 49/185, total step 49/740, training_loss = 1.0152\n",
      "epoch 1/4, step 50/185, total step 50/740, training_loss = 1.0331\n",
      "epoch 1/4, step 51/185, total step 51/740, training_loss = 1.0120\n",
      "epoch 1/4, step 52/185, total step 52/740, training_loss = 0.9876\n",
      "epoch 1/4, step 53/185, total step 53/740, training_loss = 0.9886\n",
      "epoch 1/4, step 54/185, total step 54/740, training_loss = 0.9404\n",
      "epoch 1/4, step 55/185, total step 55/740, training_loss = 0.9897\n",
      "epoch 1/4, step 56/185, total step 56/740, training_loss = 0.9762\n",
      "epoch 1/4, step 57/185, total step 57/740, training_loss = 0.9299\n",
      "epoch 1/4, step 58/185, total step 58/740, training_loss = 0.9878\n",
      "epoch 1/4, step 59/185, total step 59/740, training_loss = 0.9009\n",
      "epoch 1/4, step 60/185, total step 60/740, training_loss = 0.8772\n",
      "epoch 1/4, step 61/185, total step 61/740, training_loss = 0.9118\n",
      "epoch 1/4, step 62/185, total step 62/740, training_loss = 0.8644\n",
      "epoch 1/4, step 63/185, total step 63/740, training_loss = 0.8834\n",
      "epoch 1/4, step 64/185, total step 64/740, training_loss = 0.8762\n",
      "epoch 1/4, step 65/185, total step 65/740, training_loss = 0.8586\n",
      "epoch 1/4, step 66/185, total step 66/740, training_loss = 0.8584\n",
      "epoch 1/4, step 67/185, total step 67/740, training_loss = 0.8238\n",
      "epoch 1/4, step 68/185, total step 68/740, training_loss = 0.8153\n",
      "epoch 1/4, step 69/185, total step 69/740, training_loss = 0.7869\n",
      "epoch 1/4, step 70/185, total step 70/740, training_loss = 0.8200\n",
      "epoch 1/4, step 71/185, total step 71/740, training_loss = 0.8018\n",
      "epoch 1/4, step 72/185, total step 72/740, training_loss = 0.7979\n",
      "epoch 1/4, step 73/185, total step 73/740, training_loss = 0.7641\n",
      "epoch 1/4, step 74/185, total step 74/740, training_loss = 0.8090\n",
      "epoch 1/4, step 75/185, total step 75/740, training_loss = 0.7996\n",
      "epoch 1/4, step 76/185, total step 76/740, training_loss = 0.7371\n",
      "epoch 1/4, step 77/185, total step 77/740, training_loss = 0.7416\n",
      "epoch 1/4, step 78/185, total step 78/740, training_loss = 0.7123\n",
      "epoch 1/4, step 79/185, total step 79/740, training_loss = 0.6999\n",
      "epoch 1/4, step 80/185, total step 80/740, training_loss = 0.7040\n",
      "epoch 1/4, step 81/185, total step 81/740, training_loss = 0.7091\n",
      "epoch 1/4, step 82/185, total step 82/740, training_loss = 0.6970\n",
      "epoch 1/4, step 83/185, total step 83/740, training_loss = 0.7148\n",
      "epoch 1/4, step 84/185, total step 84/740, training_loss = 0.7393\n",
      "epoch 1/4, step 85/185, total step 85/740, training_loss = 0.7267\n",
      "epoch 1/4, step 86/185, total step 86/740, training_loss = 0.6974\n",
      "epoch 1/4, step 87/185, total step 87/740, training_loss = 0.6795\n",
      "epoch 1/4, step 88/185, total step 88/740, training_loss = 0.6460\n",
      "epoch 1/4, step 89/185, total step 89/740, training_loss = 0.6490\n",
      "epoch 1/4, step 90/185, total step 90/740, training_loss = 0.6168\n",
      "epoch 1/4, step 91/185, total step 91/740, training_loss = 0.6080\n",
      "epoch 1/4, step 92/185, total step 92/740, training_loss = 0.6193\n",
      "epoch 1/4, step 93/185, total step 93/740, training_loss = 0.6986\n",
      "epoch 1/4, step 94/185, total step 94/740, training_loss = 0.6388\n",
      "epoch 1/4, step 95/185, total step 95/740, training_loss = 0.5788\n",
      "epoch 1/4, step 96/185, total step 96/740, training_loss = 0.6054\n",
      "epoch 1/4, step 97/185, total step 97/740, training_loss = 0.5794\n",
      "epoch 1/4, step 98/185, total step 98/740, training_loss = 0.6263\n",
      "epoch 1/4, step 99/185, total step 99/740, training_loss = 0.6279\n",
      "epoch 1/4, step 100/185, total step 100/740, training_loss = 0.6176\n",
      "epoch 1/4, step 101/185, total step 101/740, training_loss = 0.5431\n",
      "epoch 1/4, step 102/185, total step 102/740, training_loss = 0.5802\n",
      "epoch 1/4, step 103/185, total step 103/740, training_loss = 0.5719\n",
      "epoch 1/4, step 104/185, total step 104/740, training_loss = 0.5332\n",
      "epoch 1/4, step 105/185, total step 105/740, training_loss = 0.5257\n",
      "epoch 1/4, step 106/185, total step 106/740, training_loss = 0.5495\n",
      "epoch 1/4, step 107/185, total step 107/740, training_loss = 0.5014\n",
      "epoch 1/4, step 108/185, total step 108/740, training_loss = 0.5378\n",
      "epoch 1/4, step 109/185, total step 109/740, training_loss = 0.4819\n",
      "epoch 1/4, step 110/185, total step 110/740, training_loss = 0.5042\n",
      "epoch 1/4, step 111/185, total step 111/740, training_loss = 0.5344\n",
      "epoch 1/4, step 112/185, total step 112/740, training_loss = 0.4731\n",
      "epoch 1/4, step 113/185, total step 113/740, training_loss = 0.5216\n",
      "epoch 1/4, step 114/185, total step 114/740, training_loss = 0.5090\n",
      "epoch 1/4, step 115/185, total step 115/740, training_loss = 0.4686\n",
      "epoch 1/4, step 116/185, total step 116/740, training_loss = 0.5075\n",
      "epoch 1/4, step 117/185, total step 117/740, training_loss = 0.4983\n",
      "epoch 1/4, step 118/185, total step 118/740, training_loss = 0.4509\n",
      "epoch 1/4, step 119/185, total step 119/740, training_loss = 0.4674\n",
      "epoch 1/4, step 120/185, total step 120/740, training_loss = 0.5023\n",
      "epoch 1/4, step 121/185, total step 121/740, training_loss = 0.4920\n",
      "epoch 1/4, step 122/185, total step 122/740, training_loss = 0.4393\n",
      "epoch 1/4, step 123/185, total step 123/740, training_loss = 0.4585\n",
      "epoch 1/4, step 124/185, total step 124/740, training_loss = 0.4362\n",
      "epoch 1/4, step 125/185, total step 125/740, training_loss = 0.5310\n",
      "epoch 1/4, step 126/185, total step 126/740, training_loss = 0.4265\n",
      "epoch 1/4, step 127/185, total step 127/740, training_loss = 0.4185\n",
      "epoch 1/4, step 128/185, total step 128/740, training_loss = 0.5054\n",
      "epoch 1/4, step 129/185, total step 129/740, training_loss = 0.4004\n",
      "epoch 1/4, step 130/185, total step 130/740, training_loss = 0.4039\n",
      "epoch 1/4, step 131/185, total step 131/740, training_loss = 0.4264\n",
      "epoch 1/4, step 132/185, total step 132/740, training_loss = 0.4492\n",
      "epoch 1/4, step 133/185, total step 133/740, training_loss = 0.4077\n",
      "epoch 1/4, step 134/185, total step 134/740, training_loss = 0.4232\n",
      "epoch 1/4, step 135/185, total step 135/740, training_loss = 0.4160\n",
      "epoch 1/4, step 136/185, total step 136/740, training_loss = 0.3716\n",
      "epoch 1/4, step 137/185, total step 137/740, training_loss = 0.3774\n",
      "epoch 1/4, step 138/185, total step 138/740, training_loss = 0.4299\n",
      "epoch 1/4, step 139/185, total step 139/740, training_loss = 0.3670\n",
      "epoch 1/4, step 140/185, total step 140/740, training_loss = 0.3802\n",
      "epoch 1/4, step 141/185, total step 141/740, training_loss = 0.3442\n",
      "epoch 1/4, step 142/185, total step 142/740, training_loss = 0.3489\n",
      "epoch 1/4, step 143/185, total step 143/740, training_loss = 0.4396\n",
      "epoch 1/4, step 144/185, total step 144/740, training_loss = 0.3931\n",
      "epoch 1/4, step 145/185, total step 145/740, training_loss = 0.3770\n",
      "epoch 1/4, step 146/185, total step 146/740, training_loss = 0.3465\n",
      "epoch 1/4, step 147/185, total step 147/740, training_loss = 0.3746\n",
      "epoch 1/4, step 148/185, total step 148/740, training_loss = 0.3562\n",
      "epoch 1/4, step 149/185, total step 149/740, training_loss = 0.4084\n",
      "epoch 1/4, step 150/185, total step 150/740, training_loss = 0.3633\n",
      "epoch 1/4, step 151/185, total step 151/740, training_loss = 0.3255\n",
      "epoch 1/4, step 152/185, total step 152/740, training_loss = 0.3209\n",
      "epoch 1/4, step 153/185, total step 153/740, training_loss = 0.3623\n",
      "epoch 1/4, step 154/185, total step 154/740, training_loss = 0.3498\n",
      "epoch 1/4, step 155/185, total step 155/740, training_loss = 0.4186\n",
      "epoch 1/4, step 156/185, total step 156/740, training_loss = 0.2873\n",
      "epoch 1/4, step 157/185, total step 157/740, training_loss = 0.3155\n",
      "epoch 1/4, step 158/185, total step 158/740, training_loss = 0.3829\n",
      "epoch 1/4, step 159/185, total step 159/740, training_loss = 0.2986\n",
      "epoch 1/4, step 160/185, total step 160/740, training_loss = 0.3109\n",
      "epoch 1/4, step 161/185, total step 161/740, training_loss = 0.3882\n",
      "epoch 1/4, step 162/185, total step 162/740, training_loss = 0.3628\n",
      "epoch 1/4, step 163/185, total step 163/740, training_loss = 0.3870\n",
      "epoch 1/4, step 164/185, total step 164/740, training_loss = 0.3541\n",
      "epoch 1/4, step 165/185, total step 165/740, training_loss = 0.3624\n",
      "epoch 1/4, step 166/185, total step 166/740, training_loss = 0.3051\n",
      "epoch 1/4, step 167/185, total step 167/740, training_loss = 0.3865\n",
      "epoch 1/4, step 168/185, total step 168/740, training_loss = 0.2999\n",
      "epoch 1/4, step 169/185, total step 169/740, training_loss = 0.3375\n",
      "epoch 1/4, step 170/185, total step 170/740, training_loss = 0.3531\n",
      "epoch 1/4, step 171/185, total step 171/740, training_loss = 0.2745\n",
      "epoch 1/4, step 172/185, total step 172/740, training_loss = 0.3762\n",
      "epoch 1/4, step 173/185, total step 173/740, training_loss = 0.3389\n",
      "epoch 1/4, step 174/185, total step 174/740, training_loss = 0.3643\n",
      "epoch 1/4, step 175/185, total step 175/740, training_loss = 0.3240\n",
      "epoch 1/4, step 176/185, total step 176/740, training_loss = 0.2699\n",
      "epoch 1/4, step 177/185, total step 177/740, training_loss = 0.2856\n",
      "epoch 1/4, step 178/185, total step 178/740, training_loss = 0.2914\n",
      "epoch 1/4, step 179/185, total step 179/740, training_loss = 0.2708\n",
      "epoch 1/4, step 180/185, total step 180/740, training_loss = 0.2841\n",
      "epoch 1/4, step 181/185, total step 181/740, training_loss = 0.3021\n",
      "epoch 1/4, step 182/185, total step 182/740, training_loss = 0.3217\n",
      "epoch 1/4, step 183/185, total step 183/740, training_loss = 0.2626\n",
      "epoch 1/4, step 184/185, total step 184/740, training_loss = 0.2491\n",
      "epoch 1/4, step 185/185, total step 185/740, training_loss = 0.2272\n",
      "evaluation for epoch 1,  rocauc = 0.9970\n",
      "epoch 2/4, step 1/185, total step 186/740, training_loss = 0.3380\n",
      "epoch 2/4, step 2/185, total step 187/740, training_loss = 0.2594\n",
      "epoch 2/4, step 3/185, total step 188/740, training_loss = 0.3189\n",
      "epoch 2/4, step 4/185, total step 189/740, training_loss = 0.2573\n",
      "epoch 2/4, step 5/185, total step 190/740, training_loss = 0.3200\n",
      "epoch 2/4, step 6/185, total step 191/740, training_loss = 0.2564\n",
      "epoch 2/4, step 7/185, total step 192/740, training_loss = 0.2565\n",
      "epoch 2/4, step 8/185, total step 193/740, training_loss = 0.2579\n",
      "epoch 2/4, step 9/185, total step 194/740, training_loss = 0.2798\n",
      "epoch 2/4, step 10/185, total step 195/740, training_loss = 0.2930\n",
      "epoch 2/4, step 11/185, total step 196/740, training_loss = 0.3252\n",
      "epoch 2/4, step 12/185, total step 197/740, training_loss = 0.2989\n",
      "epoch 2/4, step 13/185, total step 198/740, training_loss = 0.2681\n",
      "epoch 2/4, step 14/185, total step 199/740, training_loss = 0.2746\n",
      "epoch 2/4, step 15/185, total step 200/740, training_loss = 0.2516\n",
      "epoch 2/4, step 16/185, total step 201/740, training_loss = 0.2382\n",
      "epoch 2/4, step 17/185, total step 202/740, training_loss = 0.2572\n",
      "epoch 2/4, step 18/185, total step 203/740, training_loss = 0.2455\n",
      "epoch 2/4, step 19/185, total step 204/740, training_loss = 0.3126\n",
      "epoch 2/4, step 20/185, total step 205/740, training_loss = 0.2693\n",
      "epoch 2/4, step 21/185, total step 206/740, training_loss = 0.2735\n",
      "epoch 2/4, step 22/185, total step 207/740, training_loss = 0.2703\n",
      "epoch 2/4, step 23/185, total step 208/740, training_loss = 0.3000\n",
      "epoch 2/4, step 24/185, total step 209/740, training_loss = 0.2437\n",
      "epoch 2/4, step 25/185, total step 210/740, training_loss = 0.2211\n",
      "epoch 2/4, step 26/185, total step 211/740, training_loss = 0.2270\n",
      "epoch 2/4, step 27/185, total step 212/740, training_loss = 0.3003\n",
      "epoch 2/4, step 28/185, total step 213/740, training_loss = 0.2244\n",
      "epoch 2/4, step 29/185, total step 214/740, training_loss = 0.2505\n",
      "epoch 2/4, step 30/185, total step 215/740, training_loss = 0.2112\n",
      "epoch 2/4, step 31/185, total step 216/740, training_loss = 0.2466\n",
      "epoch 2/4, step 32/185, total step 217/740, training_loss = 0.2168\n",
      "epoch 2/4, step 33/185, total step 218/740, training_loss = 0.2108\n",
      "epoch 2/4, step 34/185, total step 219/740, training_loss = 0.2772\n",
      "epoch 2/4, step 35/185, total step 220/740, training_loss = 0.2303\n",
      "epoch 2/4, step 36/185, total step 221/740, training_loss = 0.1966\n",
      "epoch 2/4, step 37/185, total step 222/740, training_loss = 0.2150\n",
      "epoch 2/4, step 38/185, total step 223/740, training_loss = 0.2451\n",
      "epoch 2/4, step 39/185, total step 224/740, training_loss = 0.2422\n",
      "epoch 2/4, step 40/185, total step 225/740, training_loss = 0.2245\n",
      "epoch 2/4, step 41/185, total step 226/740, training_loss = 0.2002\n",
      "epoch 2/4, step 42/185, total step 227/740, training_loss = 0.2296\n",
      "epoch 2/4, step 43/185, total step 228/740, training_loss = 0.2325\n",
      "epoch 2/4, step 44/185, total step 229/740, training_loss = 0.2287\n",
      "epoch 2/4, step 45/185, total step 230/740, training_loss = 0.2569\n",
      "epoch 2/4, step 46/185, total step 231/740, training_loss = 0.2894\n",
      "epoch 2/4, step 47/185, total step 232/740, training_loss = 0.1844\n",
      "epoch 2/4, step 48/185, total step 233/740, training_loss = 0.2056\n",
      "epoch 2/4, step 49/185, total step 234/740, training_loss = 0.1901\n",
      "epoch 2/4, step 50/185, total step 235/740, training_loss = 0.2024\n",
      "epoch 2/4, step 51/185, total step 236/740, training_loss = 0.1910\n",
      "epoch 2/4, step 52/185, total step 237/740, training_loss = 0.1828\n",
      "epoch 2/4, step 53/185, total step 238/740, training_loss = 0.2270\n",
      "epoch 2/4, step 54/185, total step 239/740, training_loss = 0.2382\n",
      "epoch 2/4, step 55/185, total step 240/740, training_loss = 0.2239\n",
      "epoch 2/4, step 56/185, total step 241/740, training_loss = 0.2508\n",
      "epoch 2/4, step 57/185, total step 242/740, training_loss = 0.2366\n",
      "epoch 2/4, step 58/185, total step 243/740, training_loss = 0.2153\n",
      "epoch 2/4, step 59/185, total step 244/740, training_loss = 0.1872\n",
      "epoch 2/4, step 60/185, total step 245/740, training_loss = 0.1913\n",
      "epoch 2/4, step 61/185, total step 246/740, training_loss = 0.1794\n",
      "epoch 2/4, step 62/185, total step 247/740, training_loss = 0.2302\n",
      "epoch 2/4, step 63/185, total step 248/740, training_loss = 0.2227\n",
      "epoch 2/4, step 64/185, total step 249/740, training_loss = 0.1715\n",
      "epoch 2/4, step 65/185, total step 250/740, training_loss = 0.2061\n",
      "epoch 2/4, step 66/185, total step 251/740, training_loss = 0.1779\n",
      "epoch 2/4, step 67/185, total step 252/740, training_loss = 0.2005\n",
      "epoch 2/4, step 68/185, total step 253/740, training_loss = 0.2095\n",
      "epoch 2/4, step 69/185, total step 254/740, training_loss = 0.1909\n",
      "epoch 2/4, step 70/185, total step 255/740, training_loss = 0.1870\n",
      "epoch 2/4, step 71/185, total step 256/740, training_loss = 0.1928\n",
      "epoch 2/4, step 72/185, total step 257/740, training_loss = 0.1852\n",
      "epoch 2/4, step 73/185, total step 258/740, training_loss = 0.1895\n",
      "epoch 2/4, step 74/185, total step 259/740, training_loss = 0.1953\n",
      "epoch 2/4, step 75/185, total step 260/740, training_loss = 0.2115\n",
      "epoch 2/4, step 76/185, total step 261/740, training_loss = 0.1867\n",
      "epoch 2/4, step 77/185, total step 262/740, training_loss = 0.1885\n",
      "epoch 2/4, step 78/185, total step 263/740, training_loss = 0.1845\n",
      "epoch 2/4, step 79/185, total step 264/740, training_loss = 0.1522\n",
      "epoch 2/4, step 80/185, total step 265/740, training_loss = 0.1478\n",
      "epoch 2/4, step 81/185, total step 266/740, training_loss = 0.1698\n",
      "epoch 2/4, step 82/185, total step 267/740, training_loss = 0.1605\n",
      "epoch 2/4, step 83/185, total step 268/740, training_loss = 0.1575\n",
      "epoch 2/4, step 84/185, total step 269/740, training_loss = 0.1805\n",
      "epoch 2/4, step 85/185, total step 270/740, training_loss = 0.1542\n",
      "epoch 2/4, step 86/185, total step 271/740, training_loss = 0.2230\n",
      "epoch 2/4, step 87/185, total step 272/740, training_loss = 0.1660\n",
      "epoch 2/4, step 88/185, total step 273/740, training_loss = 0.1456\n",
      "epoch 2/4, step 89/185, total step 274/740, training_loss = 0.1824\n",
      "epoch 2/4, step 90/185, total step 275/740, training_loss = 0.1878\n",
      "epoch 2/4, step 91/185, total step 276/740, training_loss = 0.1861\n",
      "epoch 2/4, step 92/185, total step 277/740, training_loss = 0.1824\n",
      "epoch 2/4, step 93/185, total step 278/740, training_loss = 0.1929\n",
      "epoch 2/4, step 94/185, total step 279/740, training_loss = 0.1795\n",
      "epoch 2/4, step 95/185, total step 280/740, training_loss = 0.1891\n",
      "epoch 2/4, step 96/185, total step 281/740, training_loss = 0.1739\n",
      "epoch 2/4, step 97/185, total step 282/740, training_loss = 0.1467\n",
      "epoch 2/4, step 98/185, total step 283/740, training_loss = 0.1600\n",
      "epoch 2/4, step 99/185, total step 284/740, training_loss = 0.1789\n",
      "epoch 2/4, step 100/185, total step 285/740, training_loss = 0.1615\n",
      "epoch 2/4, step 101/185, total step 286/740, training_loss = 0.1803\n",
      "epoch 2/4, step 102/185, total step 287/740, training_loss = 0.1448\n",
      "epoch 2/4, step 103/185, total step 288/740, training_loss = 0.1248\n",
      "epoch 2/4, step 104/185, total step 289/740, training_loss = 0.1959\n",
      "epoch 2/4, step 105/185, total step 290/740, training_loss = 0.1752\n",
      "epoch 2/4, step 106/185, total step 291/740, training_loss = 0.1516\n",
      "epoch 2/4, step 107/185, total step 292/740, training_loss = 0.1558\n",
      "epoch 2/4, step 108/185, total step 293/740, training_loss = 0.1756\n",
      "epoch 2/4, step 109/185, total step 294/740, training_loss = 0.1292\n",
      "epoch 2/4, step 110/185, total step 295/740, training_loss = 0.1798\n",
      "epoch 2/4, step 111/185, total step 296/740, training_loss = 0.1320\n",
      "epoch 2/4, step 112/185, total step 297/740, training_loss = 0.1269\n",
      "epoch 2/4, step 113/185, total step 298/740, training_loss = 0.1487\n",
      "epoch 2/4, step 114/185, total step 299/740, training_loss = 0.1791\n",
      "epoch 2/4, step 115/185, total step 300/740, training_loss = 0.1482\n",
      "epoch 2/4, step 116/185, total step 301/740, training_loss = 0.1695\n",
      "epoch 2/4, step 117/185, total step 302/740, training_loss = 0.1612\n",
      "epoch 2/4, step 118/185, total step 303/740, training_loss = 0.1146\n",
      "epoch 2/4, step 119/185, total step 304/740, training_loss = 0.1527\n",
      "epoch 2/4, step 120/185, total step 305/740, training_loss = 0.1550\n",
      "epoch 2/4, step 121/185, total step 306/740, training_loss = 0.1486\n",
      "epoch 2/4, step 122/185, total step 307/740, training_loss = 0.1587\n",
      "epoch 2/4, step 123/185, total step 308/740, training_loss = 0.1454\n",
      "epoch 2/4, step 124/185, total step 309/740, training_loss = 0.1179\n",
      "epoch 2/4, step 125/185, total step 310/740, training_loss = 0.1344\n",
      "epoch 2/4, step 126/185, total step 311/740, training_loss = 0.1174\n",
      "epoch 2/4, step 127/185, total step 312/740, training_loss = 0.1502\n",
      "epoch 2/4, step 128/185, total step 313/740, training_loss = 0.1300\n",
      "epoch 2/4, step 129/185, total step 314/740, training_loss = 0.1594\n",
      "epoch 2/4, step 130/185, total step 315/740, training_loss = 0.1195\n",
      "epoch 2/4, step 131/185, total step 316/740, training_loss = 0.1403\n",
      "epoch 2/4, step 132/185, total step 317/740, training_loss = 0.1540\n",
      "epoch 2/4, step 133/185, total step 318/740, training_loss = 0.1274\n",
      "epoch 2/4, step 134/185, total step 319/740, training_loss = 0.1758\n",
      "epoch 2/4, step 135/185, total step 320/740, training_loss = 0.1260\n",
      "epoch 2/4, step 136/185, total step 321/740, training_loss = 0.1262\n",
      "epoch 2/4, step 137/185, total step 322/740, training_loss = 0.1231\n",
      "epoch 2/4, step 138/185, total step 323/740, training_loss = 0.1255\n",
      "epoch 2/4, step 139/185, total step 324/740, training_loss = 0.1221\n",
      "epoch 2/4, step 140/185, total step 325/740, training_loss = 0.1321\n",
      "epoch 2/4, step 141/185, total step 326/740, training_loss = 0.1584\n",
      "epoch 2/4, step 142/185, total step 327/740, training_loss = 0.1270\n",
      "epoch 2/4, step 143/185, total step 328/740, training_loss = 0.1544\n",
      "epoch 2/4, step 144/185, total step 329/740, training_loss = 0.1544\n",
      "epoch 2/4, step 145/185, total step 330/740, training_loss = 0.1296\n",
      "epoch 2/4, step 146/185, total step 331/740, training_loss = 0.1224\n",
      "epoch 2/4, step 147/185, total step 332/740, training_loss = 0.1428\n",
      "epoch 2/4, step 148/185, total step 333/740, training_loss = 0.1206\n",
      "epoch 2/4, step 149/185, total step 334/740, training_loss = 0.1183\n",
      "epoch 2/4, step 150/185, total step 335/740, training_loss = 0.1144\n",
      "epoch 2/4, step 151/185, total step 336/740, training_loss = 0.1322\n",
      "epoch 2/4, step 152/185, total step 337/740, training_loss = 0.1468\n",
      "epoch 2/4, step 153/185, total step 338/740, training_loss = 0.1471\n",
      "epoch 2/4, step 154/185, total step 339/740, training_loss = 0.1163\n",
      "epoch 2/4, step 155/185, total step 340/740, training_loss = 0.1144\n",
      "epoch 2/4, step 156/185, total step 341/740, training_loss = 0.1200\n",
      "epoch 2/4, step 157/185, total step 342/740, training_loss = 0.1272\n",
      "epoch 2/4, step 158/185, total step 343/740, training_loss = 0.1121\n",
      "epoch 2/4, step 159/185, total step 344/740, training_loss = 0.1017\n",
      "epoch 2/4, step 160/185, total step 345/740, training_loss = 0.1006\n",
      "epoch 2/4, step 161/185, total step 346/740, training_loss = 0.1136\n",
      "epoch 2/4, step 162/185, total step 347/740, training_loss = 0.1372\n",
      "epoch 2/4, step 163/185, total step 348/740, training_loss = 0.1032\n",
      "epoch 2/4, step 164/185, total step 349/740, training_loss = 0.1483\n",
      "epoch 2/4, step 165/185, total step 350/740, training_loss = 0.1445\n",
      "epoch 2/4, step 166/185, total step 351/740, training_loss = 0.1307\n",
      "epoch 2/4, step 167/185, total step 352/740, training_loss = 0.1360\n",
      "epoch 2/4, step 168/185, total step 353/740, training_loss = 0.1068\n",
      "epoch 2/4, step 169/185, total step 354/740, training_loss = 0.0996\n",
      "epoch 2/4, step 170/185, total step 355/740, training_loss = 0.0986\n",
      "epoch 2/4, step 171/185, total step 356/740, training_loss = 0.1145\n",
      "epoch 2/4, step 172/185, total step 357/740, training_loss = 0.1026\n",
      "epoch 2/4, step 173/185, total step 358/740, training_loss = 0.1358\n",
      "epoch 2/4, step 174/185, total step 359/740, training_loss = 0.1135\n",
      "epoch 2/4, step 175/185, total step 360/740, training_loss = 0.0908\n",
      "epoch 2/4, step 176/185, total step 361/740, training_loss = 0.1196\n",
      "epoch 2/4, step 177/185, total step 362/740, training_loss = 0.1044\n",
      "epoch 2/4, step 178/185, total step 363/740, training_loss = 0.1359\n",
      "epoch 2/4, step 179/185, total step 364/740, training_loss = 0.1031\n",
      "epoch 2/4, step 180/185, total step 365/740, training_loss = 0.1603\n",
      "epoch 2/4, step 181/185, total step 366/740, training_loss = 0.1226\n",
      "epoch 2/4, step 182/185, total step 367/740, training_loss = 0.1184\n",
      "epoch 2/4, step 183/185, total step 368/740, training_loss = 0.0982\n",
      "epoch 2/4, step 184/185, total step 369/740, training_loss = 0.1274\n",
      "epoch 2/4, step 185/185, total step 370/740, training_loss = 0.2959\n",
      "evaluation for epoch 2,  rocauc = 0.9997\n",
      "epoch 3/4, step 1/185, total step 371/740, training_loss = 0.1127\n",
      "epoch 3/4, step 2/185, total step 372/740, training_loss = 0.1035\n",
      "epoch 3/4, step 3/185, total step 373/740, training_loss = 0.0961\n",
      "epoch 3/4, step 4/185, total step 374/740, training_loss = 0.1046\n",
      "epoch 3/4, step 5/185, total step 375/740, training_loss = 0.1107\n",
      "epoch 3/4, step 6/185, total step 376/740, training_loss = 0.0937\n",
      "epoch 3/4, step 7/185, total step 377/740, training_loss = 0.0917\n",
      "epoch 3/4, step 8/185, total step 378/740, training_loss = 0.1080\n",
      "epoch 3/4, step 9/185, total step 379/740, training_loss = 0.1139\n",
      "epoch 3/4, step 10/185, total step 380/740, training_loss = 0.0859\n",
      "epoch 3/4, step 11/185, total step 381/740, training_loss = 0.1131\n",
      "epoch 3/4, step 12/185, total step 382/740, training_loss = 0.1134\n",
      "epoch 3/4, step 13/185, total step 383/740, training_loss = 0.1354\n",
      "epoch 3/4, step 14/185, total step 384/740, training_loss = 0.0957\n",
      "epoch 3/4, step 15/185, total step 385/740, training_loss = 0.1429\n",
      "epoch 3/4, step 16/185, total step 386/740, training_loss = 0.1159\n",
      "epoch 3/4, step 17/185, total step 387/740, training_loss = 0.1174\n",
      "epoch 3/4, step 18/185, total step 388/740, training_loss = 0.1174\n",
      "epoch 3/4, step 19/185, total step 389/740, training_loss = 0.1066\n",
      "epoch 3/4, step 20/185, total step 390/740, training_loss = 0.0965\n",
      "epoch 3/4, step 21/185, total step 391/740, training_loss = 0.0970\n",
      "epoch 3/4, step 22/185, total step 392/740, training_loss = 0.0865\n",
      "epoch 3/4, step 23/185, total step 393/740, training_loss = 0.0901\n",
      "epoch 3/4, step 24/185, total step 394/740, training_loss = 0.0994\n",
      "epoch 3/4, step 25/185, total step 395/740, training_loss = 0.1532\n",
      "epoch 3/4, step 26/185, total step 396/740, training_loss = 0.0957\n",
      "epoch 3/4, step 27/185, total step 397/740, training_loss = 0.0879\n",
      "epoch 3/4, step 28/185, total step 398/740, training_loss = 0.1059\n",
      "epoch 3/4, step 29/185, total step 399/740, training_loss = 0.0882\n",
      "epoch 3/4, step 30/185, total step 400/740, training_loss = 0.1035\n",
      "epoch 3/4, step 31/185, total step 401/740, training_loss = 0.0859\n",
      "epoch 3/4, step 32/185, total step 402/740, training_loss = 0.0849\n",
      "epoch 3/4, step 33/185, total step 403/740, training_loss = 0.1161\n",
      "epoch 3/4, step 34/185, total step 404/740, training_loss = 0.0949\n",
      "epoch 3/4, step 35/185, total step 405/740, training_loss = 0.1058\n",
      "epoch 3/4, step 36/185, total step 406/740, training_loss = 0.1310\n",
      "epoch 3/4, step 37/185, total step 407/740, training_loss = 0.1094\n",
      "epoch 3/4, step 38/185, total step 408/740, training_loss = 0.0824\n",
      "epoch 3/4, step 39/185, total step 409/740, training_loss = 0.1013\n",
      "epoch 3/4, step 40/185, total step 410/740, training_loss = 0.1255\n",
      "epoch 3/4, step 41/185, total step 411/740, training_loss = 0.1174\n",
      "epoch 3/4, step 42/185, total step 412/740, training_loss = 0.0822\n",
      "epoch 3/4, step 43/185, total step 413/740, training_loss = 0.1139\n",
      "epoch 3/4, step 44/185, total step 414/740, training_loss = 0.0746\n",
      "epoch 3/4, step 45/185, total step 415/740, training_loss = 0.1124\n",
      "epoch 3/4, step 46/185, total step 416/740, training_loss = 0.0781\n",
      "epoch 3/4, step 47/185, total step 417/740, training_loss = 0.1075\n",
      "epoch 3/4, step 48/185, total step 418/740, training_loss = 0.0791\n",
      "epoch 3/4, step 49/185, total step 419/740, training_loss = 0.0751\n",
      "epoch 3/4, step 50/185, total step 420/740, training_loss = 0.0942\n",
      "epoch 3/4, step 51/185, total step 421/740, training_loss = 0.1038\n",
      "epoch 3/4, step 52/185, total step 422/740, training_loss = 0.1273\n",
      "epoch 3/4, step 53/185, total step 423/740, training_loss = 0.0889\n",
      "epoch 3/4, step 54/185, total step 424/740, training_loss = 0.1200\n",
      "epoch 3/4, step 55/185, total step 425/740, training_loss = 0.0843\n",
      "epoch 3/4, step 56/185, total step 426/740, training_loss = 0.1269\n",
      "epoch 3/4, step 57/185, total step 427/740, training_loss = 0.1206\n",
      "epoch 3/4, step 58/185, total step 428/740, training_loss = 0.1060\n",
      "epoch 3/4, step 59/185, total step 429/740, training_loss = 0.0927\n",
      "epoch 3/4, step 60/185, total step 430/740, training_loss = 0.1044\n",
      "epoch 3/4, step 61/185, total step 431/740, training_loss = 0.1018\n",
      "epoch 3/4, step 62/185, total step 432/740, training_loss = 0.0908\n",
      "epoch 3/4, step 63/185, total step 433/740, training_loss = 0.0811\n",
      "epoch 3/4, step 64/185, total step 434/740, training_loss = 0.1115\n",
      "epoch 3/4, step 65/185, total step 435/740, training_loss = 0.1039\n",
      "epoch 3/4, step 66/185, total step 436/740, training_loss = 0.0916\n",
      "epoch 3/4, step 67/185, total step 437/740, training_loss = 0.0918\n",
      "epoch 3/4, step 68/185, total step 438/740, training_loss = 0.0895\n",
      "epoch 3/4, step 69/185, total step 439/740, training_loss = 0.0919\n",
      "epoch 3/4, step 70/185, total step 440/740, training_loss = 0.1505\n",
      "epoch 3/4, step 71/185, total step 441/740, training_loss = 0.0901\n",
      "epoch 3/4, step 72/185, total step 442/740, training_loss = 0.1005\n",
      "epoch 3/4, step 73/185, total step 443/740, training_loss = 0.0829\n",
      "epoch 3/4, step 74/185, total step 444/740, training_loss = 0.0974\n",
      "epoch 3/4, step 75/185, total step 445/740, training_loss = 0.0634\n",
      "epoch 3/4, step 76/185, total step 446/740, training_loss = 0.0706\n",
      "epoch 3/4, step 77/185, total step 447/740, training_loss = 0.0819\n",
      "epoch 3/4, step 78/185, total step 448/740, training_loss = 0.0790\n",
      "epoch 3/4, step 79/185, total step 449/740, training_loss = 0.0845\n",
      "epoch 3/4, step 80/185, total step 450/740, training_loss = 0.0942\n",
      "epoch 3/4, step 81/185, total step 451/740, training_loss = 0.0854\n",
      "epoch 3/4, step 82/185, total step 452/740, training_loss = 0.0765\n",
      "epoch 3/4, step 83/185, total step 453/740, training_loss = 0.0648\n",
      "epoch 3/4, step 84/185, total step 454/740, training_loss = 0.0866\n",
      "epoch 3/4, step 85/185, total step 455/740, training_loss = 0.0628\n",
      "epoch 3/4, step 86/185, total step 456/740, training_loss = 0.0834\n",
      "epoch 3/4, step 87/185, total step 457/740, training_loss = 0.0615\n",
      "epoch 3/4, step 88/185, total step 458/740, training_loss = 0.0812\n",
      "epoch 3/4, step 89/185, total step 459/740, training_loss = 0.0922\n",
      "epoch 3/4, step 90/185, total step 460/740, training_loss = 0.0582\n",
      "epoch 3/4, step 91/185, total step 461/740, training_loss = 0.1113\n",
      "epoch 3/4, step 92/185, total step 462/740, training_loss = 0.0880\n",
      "epoch 3/4, step 93/185, total step 463/740, training_loss = 0.0627\n",
      "epoch 3/4, step 94/185, total step 464/740, training_loss = 0.1164\n",
      "epoch 3/4, step 95/185, total step 465/740, training_loss = 0.0835\n",
      "epoch 3/4, step 96/185, total step 466/740, training_loss = 0.1043\n",
      "epoch 3/4, step 97/185, total step 467/740, training_loss = 0.0719\n",
      "epoch 3/4, step 98/185, total step 468/740, training_loss = 0.0613\n",
      "epoch 3/4, step 99/185, total step 469/740, training_loss = 0.0576\n",
      "epoch 3/4, step 100/185, total step 470/740, training_loss = 0.0747\n",
      "epoch 3/4, step 101/185, total step 471/740, training_loss = 0.0888\n",
      "epoch 3/4, step 102/185, total step 472/740, training_loss = 0.0708\n",
      "epoch 3/4, step 103/185, total step 473/740, training_loss = 0.0679\n",
      "epoch 3/4, step 104/185, total step 474/740, training_loss = 0.0712\n",
      "epoch 3/4, step 105/185, total step 475/740, training_loss = 0.0638\n",
      "epoch 3/4, step 106/185, total step 476/740, training_loss = 0.0658\n",
      "epoch 3/4, step 107/185, total step 477/740, training_loss = 0.0754\n",
      "epoch 3/4, step 108/185, total step 478/740, training_loss = 0.0633\n",
      "epoch 3/4, step 109/185, total step 479/740, training_loss = 0.0637\n",
      "epoch 3/4, step 110/185, total step 480/740, training_loss = 0.0655\n",
      "epoch 3/4, step 111/185, total step 481/740, training_loss = 0.0630\n",
      "epoch 3/4, step 112/185, total step 482/740, training_loss = 0.0691\n",
      "epoch 3/4, step 113/185, total step 483/740, training_loss = 0.0669\n",
      "epoch 3/4, step 114/185, total step 484/740, training_loss = 0.0801\n",
      "epoch 3/4, step 115/185, total step 485/740, training_loss = 0.0508\n",
      "epoch 3/4, step 116/185, total step 486/740, training_loss = 0.1183\n",
      "epoch 3/4, step 117/185, total step 487/740, training_loss = 0.0644\n",
      "epoch 3/4, step 118/185, total step 488/740, training_loss = 0.0595\n",
      "epoch 3/4, step 119/185, total step 489/740, training_loss = 0.0668\n",
      "epoch 3/4, step 120/185, total step 490/740, training_loss = 0.0769\n",
      "epoch 3/4, step 121/185, total step 491/740, training_loss = 0.0944\n",
      "epoch 3/4, step 122/185, total step 492/740, training_loss = 0.0539\n",
      "epoch 3/4, step 123/185, total step 493/740, training_loss = 0.0618\n",
      "epoch 3/4, step 124/185, total step 494/740, training_loss = 0.0973\n",
      "epoch 3/4, step 125/185, total step 495/740, training_loss = 0.0538\n",
      "epoch 3/4, step 126/185, total step 496/740, training_loss = 0.0945\n",
      "epoch 3/4, step 127/185, total step 497/740, training_loss = 0.0754\n",
      "epoch 3/4, step 128/185, total step 498/740, training_loss = 0.0805\n",
      "epoch 3/4, step 129/185, total step 499/740, training_loss = 0.0707\n",
      "epoch 3/4, step 130/185, total step 500/740, training_loss = 0.0640\n",
      "epoch 3/4, step 131/185, total step 501/740, training_loss = 0.0746\n",
      "epoch 3/4, step 132/185, total step 502/740, training_loss = 0.0498\n",
      "epoch 3/4, step 133/185, total step 503/740, training_loss = 0.0732\n",
      "epoch 3/4, step 134/185, total step 504/740, training_loss = 0.0743\n",
      "epoch 3/4, step 135/185, total step 505/740, training_loss = 0.0595\n",
      "epoch 3/4, step 136/185, total step 506/740, training_loss = 0.0654\n",
      "epoch 3/4, step 137/185, total step 507/740, training_loss = 0.0734\n",
      "epoch 3/4, step 138/185, total step 508/740, training_loss = 0.0913\n",
      "epoch 3/4, step 139/185, total step 509/740, training_loss = 0.0596\n",
      "epoch 3/4, step 140/185, total step 510/740, training_loss = 0.0715\n",
      "epoch 3/4, step 141/185, total step 511/740, training_loss = 0.0704\n",
      "epoch 3/4, step 142/185, total step 512/740, training_loss = 0.0650\n",
      "epoch 3/4, step 143/185, total step 513/740, training_loss = 0.0590\n",
      "epoch 3/4, step 144/185, total step 514/740, training_loss = 0.0641\n",
      "epoch 3/4, step 145/185, total step 515/740, training_loss = 0.0570\n",
      "epoch 3/4, step 146/185, total step 516/740, training_loss = 0.0715\n",
      "epoch 3/4, step 147/185, total step 517/740, training_loss = 0.0778\n",
      "epoch 3/4, step 148/185, total step 518/740, training_loss = 0.0630\n",
      "epoch 3/4, step 149/185, total step 519/740, training_loss = 0.0461\n",
      "epoch 3/4, step 150/185, total step 520/740, training_loss = 0.0501\n",
      "epoch 3/4, step 151/185, total step 521/740, training_loss = 0.0516\n",
      "epoch 3/4, step 152/185, total step 522/740, training_loss = 0.0642\n",
      "epoch 3/4, step 153/185, total step 523/740, training_loss = 0.0656\n",
      "epoch 3/4, step 154/185, total step 524/740, training_loss = 0.0537\n",
      "epoch 3/4, step 155/185, total step 525/740, training_loss = 0.0874\n",
      "epoch 3/4, step 156/185, total step 526/740, training_loss = 0.0747\n",
      "epoch 3/4, step 157/185, total step 527/740, training_loss = 0.0685\n",
      "epoch 3/4, step 158/185, total step 528/740, training_loss = 0.0783\n",
      "epoch 3/4, step 159/185, total step 529/740, training_loss = 0.0788\n",
      "epoch 3/4, step 160/185, total step 530/740, training_loss = 0.0805\n",
      "epoch 3/4, step 161/185, total step 531/740, training_loss = 0.0838\n",
      "epoch 3/4, step 162/185, total step 532/740, training_loss = 0.0918\n",
      "epoch 3/4, step 163/185, total step 533/740, training_loss = 0.0471\n",
      "epoch 3/4, step 164/185, total step 534/740, training_loss = 0.0694\n",
      "epoch 3/4, step 165/185, total step 535/740, training_loss = 0.0745\n",
      "epoch 3/4, step 166/185, total step 536/740, training_loss = 0.0622\n",
      "epoch 3/4, step 167/185, total step 537/740, training_loss = 0.0501\n",
      "epoch 3/4, step 168/185, total step 538/740, training_loss = 0.0749\n",
      "epoch 3/4, step 169/185, total step 539/740, training_loss = 0.0743\n",
      "epoch 3/4, step 170/185, total step 540/740, training_loss = 0.0709\n",
      "epoch 3/4, step 171/185, total step 541/740, training_loss = 0.0863\n",
      "epoch 3/4, step 172/185, total step 542/740, training_loss = 0.0509\n",
      "epoch 3/4, step 173/185, total step 543/740, training_loss = 0.0366\n",
      "epoch 3/4, step 174/185, total step 544/740, training_loss = 0.0552\n",
      "epoch 3/4, step 175/185, total step 545/740, training_loss = 0.0569\n",
      "epoch 3/4, step 176/185, total step 546/740, training_loss = 0.0796\n",
      "epoch 3/4, step 177/185, total step 547/740, training_loss = 0.0774\n",
      "epoch 3/4, step 178/185, total step 548/740, training_loss = 0.0578\n",
      "epoch 3/4, step 179/185, total step 549/740, training_loss = 0.0741\n",
      "epoch 3/4, step 180/185, total step 550/740, training_loss = 0.0561\n",
      "epoch 3/4, step 181/185, total step 551/740, training_loss = 0.0491\n",
      "epoch 3/4, step 182/185, total step 552/740, training_loss = 0.0563\n",
      "epoch 3/4, step 183/185, total step 553/740, training_loss = 0.0610\n",
      "epoch 3/4, step 184/185, total step 554/740, training_loss = 0.0684\n",
      "epoch 3/4, step 185/185, total step 555/740, training_loss = 0.1519\n",
      "evaluation for epoch 3,  rocauc = 0.9999\n",
      "epoch 4/4, step 1/185, total step 556/740, training_loss = 0.0566\n",
      "epoch 4/4, step 2/185, total step 557/740, training_loss = 0.0537\n",
      "epoch 4/4, step 3/185, total step 558/740, training_loss = 0.0892\n",
      "epoch 4/4, step 4/185, total step 559/740, training_loss = 0.0484\n",
      "epoch 4/4, step 5/185, total step 560/740, training_loss = 0.0633\n",
      "epoch 4/4, step 6/185, total step 561/740, training_loss = 0.0601\n",
      "epoch 4/4, step 7/185, total step 562/740, training_loss = 0.0713\n",
      "epoch 4/4, step 8/185, total step 563/740, training_loss = 0.0615\n",
      "epoch 4/4, step 9/185, total step 564/740, training_loss = 0.0553\n",
      "epoch 4/4, step 10/185, total step 565/740, training_loss = 0.0482\n",
      "epoch 4/4, step 11/185, total step 566/740, training_loss = 0.0837\n",
      "epoch 4/4, step 12/185, total step 567/740, training_loss = 0.0525\n",
      "epoch 4/4, step 13/185, total step 568/740, training_loss = 0.0422\n",
      "epoch 4/4, step 14/185, total step 569/740, training_loss = 0.0659\n",
      "epoch 4/4, step 15/185, total step 570/740, training_loss = 0.0603\n",
      "epoch 4/4, step 16/185, total step 571/740, training_loss = 0.0577\n",
      "epoch 4/4, step 17/185, total step 572/740, training_loss = 0.0408\n",
      "epoch 4/4, step 18/185, total step 573/740, training_loss = 0.0839\n",
      "epoch 4/4, step 19/185, total step 574/740, training_loss = 0.0697\n",
      "epoch 4/4, step 20/185, total step 575/740, training_loss = 0.0787\n",
      "epoch 4/4, step 21/185, total step 576/740, training_loss = 0.0539\n",
      "epoch 4/4, step 22/185, total step 577/740, training_loss = 0.0534\n",
      "epoch 4/4, step 23/185, total step 578/740, training_loss = 0.0633\n",
      "epoch 4/4, step 24/185, total step 579/740, training_loss = 0.0546\n",
      "epoch 4/4, step 25/185, total step 580/740, training_loss = 0.0938\n",
      "epoch 4/4, step 26/185, total step 581/740, training_loss = 0.0496\n",
      "epoch 4/4, step 27/185, total step 582/740, training_loss = 0.0526\n",
      "epoch 4/4, step 28/185, total step 583/740, training_loss = 0.0765\n",
      "epoch 4/4, step 29/185, total step 584/740, training_loss = 0.0300\n",
      "epoch 4/4, step 30/185, total step 585/740, training_loss = 0.0421\n",
      "epoch 4/4, step 31/185, total step 586/740, training_loss = 0.0606\n",
      "epoch 4/4, step 32/185, total step 587/740, training_loss = 0.0650\n",
      "epoch 4/4, step 33/185, total step 588/740, training_loss = 0.0487\n",
      "epoch 4/4, step 34/185, total step 589/740, training_loss = 0.0673\n",
      "epoch 4/4, step 35/185, total step 590/740, training_loss = 0.0453\n",
      "epoch 4/4, step 36/185, total step 591/740, training_loss = 0.0515\n",
      "epoch 4/4, step 37/185, total step 592/740, training_loss = 0.0368\n",
      "epoch 4/4, step 38/185, total step 593/740, training_loss = 0.0481\n",
      "epoch 4/4, step 39/185, total step 594/740, training_loss = 0.0391\n",
      "epoch 4/4, step 40/185, total step 595/740, training_loss = 0.0634\n",
      "epoch 4/4, step 41/185, total step 596/740, training_loss = 0.0419\n",
      "epoch 4/4, step 42/185, total step 597/740, training_loss = 0.0567\n",
      "epoch 4/4, step 43/185, total step 598/740, training_loss = 0.0536\n",
      "epoch 4/4, step 44/185, total step 599/740, training_loss = 0.0843\n",
      "epoch 4/4, step 45/185, total step 600/740, training_loss = 0.0581\n",
      "epoch 4/4, step 46/185, total step 601/740, training_loss = 0.0634\n",
      "epoch 4/4, step 47/185, total step 602/740, training_loss = 0.0359\n",
      "epoch 4/4, step 48/185, total step 603/740, training_loss = 0.0863\n",
      "epoch 4/4, step 49/185, total step 604/740, training_loss = 0.0647\n",
      "epoch 4/4, step 50/185, total step 605/740, training_loss = 0.0544\n",
      "epoch 4/4, step 51/185, total step 606/740, training_loss = 0.0419\n",
      "epoch 4/4, step 52/185, total step 607/740, training_loss = 0.0366\n",
      "epoch 4/4, step 53/185, total step 608/740, training_loss = 0.0582\n",
      "epoch 4/4, step 54/185, total step 609/740, training_loss = 0.0377\n",
      "epoch 4/4, step 55/185, total step 610/740, training_loss = 0.0479\n",
      "epoch 4/4, step 56/185, total step 611/740, training_loss = 0.0475\n",
      "epoch 4/4, step 57/185, total step 612/740, training_loss = 0.0329\n",
      "epoch 4/4, step 58/185, total step 613/740, training_loss = 0.0410\n",
      "epoch 4/4, step 59/185, total step 614/740, training_loss = 0.0600\n",
      "epoch 4/4, step 60/185, total step 615/740, training_loss = 0.0437\n",
      "epoch 4/4, step 61/185, total step 616/740, training_loss = 0.0578\n",
      "epoch 4/4, step 62/185, total step 617/740, training_loss = 0.0358\n",
      "epoch 4/4, step 63/185, total step 618/740, training_loss = 0.0692\n",
      "epoch 4/4, step 64/185, total step 619/740, training_loss = 0.0372\n",
      "epoch 4/4, step 65/185, total step 620/740, training_loss = 0.0542\n",
      "epoch 4/4, step 66/185, total step 621/740, training_loss = 0.0783\n",
      "epoch 4/4, step 67/185, total step 622/740, training_loss = 0.0487\n",
      "epoch 4/4, step 68/185, total step 623/740, training_loss = 0.0585\n",
      "epoch 4/4, step 69/185, total step 624/740, training_loss = 0.0720\n",
      "epoch 4/4, step 70/185, total step 625/740, training_loss = 0.0717\n",
      "epoch 4/4, step 71/185, total step 626/740, training_loss = 0.0775\n",
      "epoch 4/4, step 72/185, total step 627/740, training_loss = 0.0544\n",
      "epoch 4/4, step 73/185, total step 628/740, training_loss = 0.0499\n",
      "epoch 4/4, step 74/185, total step 629/740, training_loss = 0.0734\n",
      "epoch 4/4, step 75/185, total step 630/740, training_loss = 0.0492\n",
      "epoch 4/4, step 76/185, total step 631/740, training_loss = 0.0414\n",
      "epoch 4/4, step 77/185, total step 632/740, training_loss = 0.0414\n",
      "epoch 4/4, step 78/185, total step 633/740, training_loss = 0.0621\n",
      "epoch 4/4, step 79/185, total step 634/740, training_loss = 0.0387\n",
      "epoch 4/4, step 80/185, total step 635/740, training_loss = 0.0568\n",
      "epoch 4/4, step 81/185, total step 636/740, training_loss = 0.0442\n",
      "epoch 4/4, step 82/185, total step 637/740, training_loss = 0.0786\n",
      "epoch 4/4, step 83/185, total step 638/740, training_loss = 0.0452\n",
      "epoch 4/4, step 84/185, total step 639/740, training_loss = 0.0528\n",
      "epoch 4/4, step 85/185, total step 640/740, training_loss = 0.0622\n",
      "epoch 4/4, step 86/185, total step 641/740, training_loss = 0.0469\n",
      "epoch 4/4, step 87/185, total step 642/740, training_loss = 0.0603\n",
      "epoch 4/4, step 88/185, total step 643/740, training_loss = 0.0322\n",
      "epoch 4/4, step 89/185, total step 644/740, training_loss = 0.0459\n",
      "epoch 4/4, step 90/185, total step 645/740, training_loss = 0.0579\n",
      "epoch 4/4, step 91/185, total step 646/740, training_loss = 0.0246\n",
      "epoch 4/4, step 92/185, total step 647/740, training_loss = 0.0369\n",
      "epoch 4/4, step 93/185, total step 648/740, training_loss = 0.0448\n",
      "epoch 4/4, step 94/185, total step 649/740, training_loss = 0.0498\n",
      "epoch 4/4, step 95/185, total step 650/740, training_loss = 0.0361\n",
      "epoch 4/4, step 96/185, total step 651/740, training_loss = 0.0648\n",
      "epoch 4/4, step 97/185, total step 652/740, training_loss = 0.0526\n",
      "epoch 4/4, step 98/185, total step 653/740, training_loss = 0.0631\n",
      "epoch 4/4, step 99/185, total step 654/740, training_loss = 0.0422\n",
      "epoch 4/4, step 100/185, total step 655/740, training_loss = 0.0529\n",
      "epoch 4/4, step 101/185, total step 656/740, training_loss = 0.0382\n",
      "epoch 4/4, step 102/185, total step 657/740, training_loss = 0.0710\n",
      "epoch 4/4, step 103/185, total step 658/740, training_loss = 0.0502\n",
      "epoch 4/4, step 104/185, total step 659/740, training_loss = 0.0396\n",
      "epoch 4/4, step 105/185, total step 660/740, training_loss = 0.0365\n",
      "epoch 4/4, step 106/185, total step 661/740, training_loss = 0.0527\n",
      "epoch 4/4, step 107/185, total step 662/740, training_loss = 0.0305\n",
      "epoch 4/4, step 108/185, total step 663/740, training_loss = 0.0456\n",
      "epoch 4/4, step 109/185, total step 664/740, training_loss = 0.0543\n",
      "epoch 4/4, step 110/185, total step 665/740, training_loss = 0.0489\n",
      "epoch 4/4, step 111/185, total step 666/740, training_loss = 0.0369\n",
      "epoch 4/4, step 112/185, total step 667/740, training_loss = 0.0552\n",
      "epoch 4/4, step 113/185, total step 668/740, training_loss = 0.0470\n",
      "epoch 4/4, step 114/185, total step 669/740, training_loss = 0.0772\n",
      "epoch 4/4, step 115/185, total step 670/740, training_loss = 0.0416\n",
      "epoch 4/4, step 116/185, total step 671/740, training_loss = 0.0441\n",
      "epoch 4/4, step 117/185, total step 672/740, training_loss = 0.0421\n",
      "epoch 4/4, step 118/185, total step 673/740, training_loss = 0.0596\n",
      "epoch 4/4, step 119/185, total step 674/740, training_loss = 0.0616\n",
      "epoch 4/4, step 120/185, total step 675/740, training_loss = 0.0614\n",
      "epoch 4/4, step 121/185, total step 676/740, training_loss = 0.0456\n",
      "epoch 4/4, step 122/185, total step 677/740, training_loss = 0.0573\n",
      "epoch 4/4, step 123/185, total step 678/740, training_loss = 0.0582\n",
      "epoch 4/4, step 124/185, total step 679/740, training_loss = 0.0543\n",
      "epoch 4/4, step 125/185, total step 680/740, training_loss = 0.0640\n",
      "epoch 4/4, step 126/185, total step 681/740, training_loss = 0.0509\n",
      "epoch 4/4, step 127/185, total step 682/740, training_loss = 0.0440\n",
      "epoch 4/4, step 128/185, total step 683/740, training_loss = 0.0756\n",
      "epoch 4/4, step 129/185, total step 684/740, training_loss = 0.0661\n",
      "epoch 4/4, step 130/185, total step 685/740, training_loss = 0.0407\n",
      "epoch 4/4, step 131/185, total step 686/740, training_loss = 0.0464\n",
      "epoch 4/4, step 132/185, total step 687/740, training_loss = 0.0536\n",
      "epoch 4/4, step 133/185, total step 688/740, training_loss = 0.0568\n",
      "epoch 4/4, step 134/185, total step 689/740, training_loss = 0.0356\n",
      "epoch 4/4, step 135/185, total step 690/740, training_loss = 0.0356\n",
      "epoch 4/4, step 136/185, total step 691/740, training_loss = 0.0323\n",
      "epoch 4/4, step 137/185, total step 692/740, training_loss = 0.0199\n",
      "epoch 4/4, step 138/185, total step 693/740, training_loss = 0.0612\n",
      "epoch 4/4, step 139/185, total step 694/740, training_loss = 0.0328\n",
      "epoch 4/4, step 140/185, total step 695/740, training_loss = 0.1013\n",
      "epoch 4/4, step 141/185, total step 696/740, training_loss = 0.0480\n",
      "epoch 4/4, step 142/185, total step 697/740, training_loss = 0.0498\n",
      "epoch 4/4, step 143/185, total step 698/740, training_loss = 0.0509\n",
      "epoch 4/4, step 144/185, total step 699/740, training_loss = 0.0729\n",
      "epoch 4/4, step 145/185, total step 700/740, training_loss = 0.0408\n",
      "epoch 4/4, step 146/185, total step 701/740, training_loss = 0.0401\n",
      "epoch 4/4, step 147/185, total step 702/740, training_loss = 0.0430\n",
      "epoch 4/4, step 148/185, total step 703/740, training_loss = 0.0437\n",
      "epoch 4/4, step 149/185, total step 704/740, training_loss = 0.0349\n",
      "epoch 4/4, step 150/185, total step 705/740, training_loss = 0.0273\n",
      "epoch 4/4, step 151/185, total step 706/740, training_loss = 0.0352\n",
      "epoch 4/4, step 152/185, total step 707/740, training_loss = 0.0690\n",
      "epoch 4/4, step 153/185, total step 708/740, training_loss = 0.0219\n",
      "epoch 4/4, step 154/185, total step 709/740, training_loss = 0.0300\n",
      "epoch 4/4, step 155/185, total step 710/740, training_loss = 0.0456\n",
      "epoch 4/4, step 156/185, total step 711/740, training_loss = 0.0519\n",
      "epoch 4/4, step 157/185, total step 712/740, training_loss = 0.0368\n",
      "epoch 4/4, step 158/185, total step 713/740, training_loss = 0.0293\n",
      "epoch 4/4, step 159/185, total step 714/740, training_loss = 0.0346\n",
      "epoch 4/4, step 160/185, total step 715/740, training_loss = 0.0373\n",
      "epoch 4/4, step 161/185, total step 716/740, training_loss = 0.0429\n",
      "epoch 4/4, step 162/185, total step 717/740, training_loss = 0.0291\n",
      "epoch 4/4, step 163/185, total step 718/740, training_loss = 0.0414\n",
      "epoch 4/4, step 164/185, total step 719/740, training_loss = 0.0688\n",
      "epoch 4/4, step 165/185, total step 720/740, training_loss = 0.0338\n",
      "epoch 4/4, step 166/185, total step 721/740, training_loss = 0.0318\n",
      "epoch 4/4, step 167/185, total step 722/740, training_loss = 0.0337\n",
      "epoch 4/4, step 168/185, total step 723/740, training_loss = 0.0657\n",
      "epoch 4/4, step 169/185, total step 724/740, training_loss = 0.0238\n",
      "epoch 4/4, step 170/185, total step 725/740, training_loss = 0.0361\n",
      "epoch 4/4, step 171/185, total step 726/740, training_loss = 0.0293\n",
      "epoch 4/4, step 172/185, total step 727/740, training_loss = 0.0498\n",
      "epoch 4/4, step 173/185, total step 728/740, training_loss = 0.0350\n",
      "epoch 4/4, step 174/185, total step 729/740, training_loss = 0.0270\n",
      "epoch 4/4, step 175/185, total step 730/740, training_loss = 0.0329\n",
      "epoch 4/4, step 176/185, total step 731/740, training_loss = 0.0417\n",
      "epoch 4/4, step 177/185, total step 732/740, training_loss = 0.0494\n",
      "epoch 4/4, step 178/185, total step 733/740, training_loss = 0.0464\n",
      "epoch 4/4, step 179/185, total step 734/740, training_loss = 0.0361\n",
      "epoch 4/4, step 180/185, total step 735/740, training_loss = 0.0286\n",
      "epoch 4/4, step 181/185, total step 736/740, training_loss = 0.0381\n",
      "epoch 4/4, step 182/185, total step 737/740, training_loss = 0.0292\n",
      "epoch 4/4, step 183/185, total step 738/740, training_loss = 0.0838\n",
      "epoch 4/4, step 184/185, total step 739/740, training_loss = 0.0559\n",
      "epoch 4/4, step 185/185, total step 740/740, training_loss = 0.0633\n",
      "evaluation for epoch 4,  rocauc = 1.0000\n",
      "CPU times: user 5min 3s, sys: 11 s, total: 5min 14s\n",
      "Wall time: 14min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and metric\n",
    "Once we're done training we want to visualize our Loss and Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "zY9GjuN7clDi",
    "outputId": "563dbd46-aa61-4ec9-e78b-286b9628b8ed"
   },
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val AUC\")\n",
    "x = [(i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model and understand the results\n",
    "### Evaluate the model on the test dataset\n",
    "\n",
    "After training and validation, we now have the best model as determined by the validation dataset. But now we need to evaluate the model on the test dataset to check whether the final model is robust and not over-fitting. We'll use these predictions to generate a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sVoATkhclG5",
    "outputId": "d22515fb-5b31-4893-a49f-34b5a5feaa88"
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data[0].to(device),\n",
    "            test_data[1].to(device),\n",
    "        )\n",
    "        pred = net(test_images).argmax(dim=1)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some light analytics - classification report\n",
    "\n",
    "We'll utilize scikit-learn's classification report to get the precision, recall, and f1-score for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIcAG1KPdK7T",
    "outputId": "9341ade1-cc56-4069-be54-66e53297e52c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some light analytics - confusion matrix\n",
    "\n",
    "Let's also create a confusion matrix to get a better understanding of the failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "e89fSZVSclKf",
    "outputId": "d85e4a68-9ed3-48c8-cfce-4a2e4bf5c2f5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_true, y_pred)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion_matrix(y_true, y_pred), cmap=\"terrain\", interpolation='nearest')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels(['']+class_names, rotation=270)\n",
    "ax.set_yticklabels(['']+class_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution !!!\n",
    "### please shutdown all kernels with [Kernel] menu >  [Shutdown All Kernel]  before launch next notebook\n",
    "\n",
    "## Navigation\n",
    "- [01_getting started](./01_getting.ipynb)\n",
    "\n",
    "- [02_pipeline_01](./02_pipeline_01.ipynb)\n",
    "- [02_pipeline_02](./02_pipeline_02.ipynb)\n",
    "- [02_pipeline_03](./02_pipeline_03.ipynb)\n",
    "- [02_pipeline_04](./02_pipeline_04.ipynb)\n",
    "\n",
    "- [03_brain_gan Next ](./03_brain_gan_01.ipynb)\n",
    "\n",
    "- [04_spleen_segment](./04_spleen_segment.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac baseline](./05_challenge_cardiac_baseline.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac workspace](./05_challenge_cardiac_workspace.ipynb) \n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Monai_bootcamp_01_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
