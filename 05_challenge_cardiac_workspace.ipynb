{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "#  MONAI Bootcamp\n",
    "\n",
    "# Segmentation Exercise  [workspace]\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>\n",
    "\n",
    "\n",
    "In this exercise we will segment the left ventricle of the heart in relatively small images using neural networks. \n",
    "Below is the code for setting up a segmentation network and training it. The network isn't very good, **so the exercise is to improve the quality of the segmentation by improving the network and/or the training scheme including data loading efficiency and data augmentation**. \n",
    "\n",
    "The data being used here is derived from the [Sunnybrook Cardiac Dataset](https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/) of cardiac MR images, filtered to contain only left ventricular myocardium segmentations and reduced in the XY dimensions.\n",
    "\n",
    "<img src=\"https://www.cardiacatlas.org/wp-content/uploads/2015/09/scd-mri.png\" width=400>\n",
    "\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ Challenge: Improve Results and Implementation ðŸ‘ˆ\n",
    "\n",
    "### 1. Improve Data Pipeline\n",
    "\n",
    "The pipeline currently has a very basic set of transforms. We'd want to add new transforms which will add regularization to our training process, specifically modifying the image and segmentations to make the learning problem a little harder. \n",
    "\n",
    "The dataset being used is `ArrayDataset` but we have in MONAI `NPZDictItemDataset` for loading data from Numpy's NPZ file format. Change the code to use this class instead. You'll need a different way of getting `caseIndices` and splitting the dataset using it.\n",
    "\n",
    "### 2. Improve/Replace Network\n",
    "\n",
    "As you can see we're not getting good results from our network. The training loss values are jumping around and not decreasing much anymore. The validation score has topped out at 0.25, which is really poor. \n",
    "\n",
    "It's now up to you to improve the results of our segmentation task. The things to consider changing include the network itself, how data is loaded, how batches might be composed, and what transforms we want to use from MONAI. \n",
    "\n",
    "### 3. Replace The Training Loop\n",
    "\n",
    "This notebook uses a simple training loop with validation done explicitly. Replace this with a use of the `SupervisedTrainer` class and `SupervisedEvaluator` to do the evaluation throughout the training process. The graph plotting is done simply by recording values at each iteration through the loop, you'll want to use some other mechanism to do the same thing such as using a `MetricLogger` handler object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7pcbsphZh8G"
   },
   "source": [
    "### Check GPU Support\n",
    "\n",
    "Running  `!nvidia-smi` in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
    "\n",
    "if GPU Memory Usage is no `0 MiB` shutdown all kernels and restart current kernel.\n",
    "- step1. shutdown kernel with following <b>Menu</b> > <b>Kernel</b> > <b>Shut Down All kernels </b>\n",
    "- step2. restart kernelw with following <b>Menu</b> > <b>Kernel</b> > <b>Restart Kernel</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVD7911EVcWI",
    "outputId": "2783c3a4-855a-4b2b-86c0-297b7b31b9ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  5 05:27:12 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                   On |\n",
      "| N/A   30C    P0    62W / 400W |   5142MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  0    5   0   0  |   5116MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\n",
      "|                  |      2MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0    5    0    3565716      C   /opt/conda/bin/python3.8         5097MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX4DkxZygKNP"
   },
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNnKTuzBgKZm",
    "outputId": "8b7c3607-768e-4222-d4c0-82806f5a1264"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import monai\n",
    "from monai.transforms import Compose, AddChannel, ScaleIntensity, ToTensor\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.data import ArrayDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.utils import first, progress_bar\n",
    "from monai.networks import one_hot\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "set_determinism(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up our Dataset and exploring the data\n",
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called temp directory in `~/monai-lab/temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "directory = \"temp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpNZjqg3Lq-1"
   },
   "source": [
    "## download dataset \n",
    "\n",
    "We will use preprocessed Sunnybrook Cardiac MRI dataset (2MB) at  [VPH Summer SChool 2019](https://www.vph-institute.org/events/2019-vph-summer-school.html)\n",
    "\n",
    "You can check for further research\n",
    "- Sunnybrook Cardiac MRI dataset(30MB) in [kaggle](https://www.kaggle.com/datasets/salikhussaini49/sunnybrook-cardiac-mri)\n",
    "- Sunnybrook Cardiac Data DICOM file(2.6GB) at [Cardiac Atlas Project](https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "DATA_NPZ = \"https://github.com/ericspod/VPHSummerSchool2019/raw/master/scd_lvsegs.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data from the remote source and visualize a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.8 ms, sys: 22.2 ms, total: 51 ms\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "remote_file = urlopen(DATA_NPZ)\n",
    "npz = BytesIO(remote_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 64, 64) (420, 64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7a92e7cfd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load(npz)  # load all the data from the archive\n",
    "\n",
    "images = data[\"images\"]  # images in BHW array order\n",
    "segs = data[\"segs\"]  # segmentations in BHW array order\n",
    "case_indices = data[\"caseIndices\"]  # the indices in `images` for each case\n",
    "\n",
    "images = images.astype(np.float32) / images.max()  # normalize images\n",
    "\n",
    "print(images.shape, segs.shape)\n",
    "plt.imshow(images[13] + segs[13] * 0.25, cmap=\"gray\")  # show image 13 with segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our data into a training and validation set by keeping the last 6 cases as the latter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline [start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "valid_index = case_indices[-6, 0]  # keep the last 6 cases for testing\n",
    "\n",
    "# divide the images, segmentations, and categories into train/test sets\n",
    "train_images, train_segs = images[:valid_index], segs[:valid_index]\n",
    "valid_images, valid_segs = images[valid_index:], segs[valid_index:]\n",
    "\n",
    "batch_size = 50\n",
    "num_workers = 2\n",
    "\n",
    "image_trans = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),  # rescale image data to range [0,1]\n",
    "        AddChannel(),  # add 1-size channel dimension\n",
    "        ToTensor(),  # convert to tensor\n",
    "    ]\n",
    ")\n",
    "\n",
    "seg_trans = Compose([AddChannel(), ToTensor()])\n",
    "\n",
    "ds = ArrayDataset(train_images, image_trans, train_segs, seg_trans)\n",
    "loader = DataLoader(\n",
    "    dataset=ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_ds = ArrayDataset(valid_images, image_trans, valid_segs, seg_trans)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "im, seg = first(loader)\n",
    "print(im.shape, im.min(), im.max(), seg.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a MONAI data loading object to compose batches during training, and another for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = case_indices[-6, 0]  # keep the last 6 cases for testing\n",
    "\n",
    "# divide the images, segmentations, and categories into train/test sets\n",
    "train_images, train_segs = images[:valid_index], segs[:valid_index]\n",
    "valid_images, valid_segs = images[valid_index:], segs[valid_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1, 64, 64]) tensor(0.) tensor(1.) torch.Size([30, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_workers = 2\n",
    "\n",
    "image_trans = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),  # rescale image data to range [0,1]\n",
    "        AddChannel(),  # add 1-size channel dimension\n",
    "        ToTensor(),  # convert to tensor\n",
    "    ]\n",
    ")\n",
    "\n",
    "seg_trans = Compose([AddChannel(), ToTensor()])\n",
    "\n",
    "ds = ArrayDataset(train_images, image_trans, train_segs, seg_trans)\n",
    "loader = DataLoader(\n",
    "    dataset=ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_ds = ArrayDataset(valid_images, image_trans, valid_segs, seg_trans)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "im, seg = first(loader)\n",
    "print(im.shape, im.min(), im.max(), seg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline [end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Improve Data Pipeline\n",
    "\n",
    "The pipeline currently has a very basic set of transforms. We'd want to add new transforms which will add regularization to our training process, specifically modifying the image and segmentations to make the learning problem a little harder. \n",
    "\n",
    "The dataset being used is `ArrayDataset` but we have in MONAI `NPZDictItemDataset` for loading data from Numpy's NPZ file format. Change the code to use this class instead. You'll need a different way of getting `caseIndices` and splitting the dataset using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    Activationsd, \n",
    "    AsDiscreted,\n",
    "    AddChanneld,\n",
    "    ScaleIntensityd,\n",
    "    CastToTyped,\n",
    "    EnsureTyped,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandZoomd,\n",
    "    Rand2DElasticd,\n",
    "    RandAffined,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new imports\n",
    "from monai.data import  NPZDictItemDataset #TODO  #ArrayDataset  to NPZDictItemDataset\n",
    "\n",
    "############\n",
    "batch_size = 30\n",
    "num_workers = 2\n",
    "aug_prob = 0.5\n",
    "############\n",
    "from monai.transforms import (\n",
    "    Activationsd, \n",
    "    AsDiscreted,\n",
    "    AddChanneld,\n",
    "    ScaleIntensityd,\n",
    "    CastToTyped,\n",
    "    EnsureTyped,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandZoomd,\n",
    "    Rand2DElasticd,\n",
    "    RandAffined,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need a different way of getting caseIndices and splitting the dataset using it. In the baseline, We split our data into a training and validation set by keeping the last 6 cases as the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# use these when interpolating binary segmentations to ensure values are 0 or 1 only\n",
    "zoom_mode = monai.utils.enums.InterpolateMode.NEAREST\n",
    "elast_mode = monai.utils.enums.GridSampleMode.BILINEAR, monai.utils.enums.GridSampleMode.NEAREST\n",
    "\n",
    "#########\n",
    "# for keys \n",
    "from monai.utils.enums import CommonKeys\n",
    "both_keys = (None, None) # TODO (CommonKeys.IMAGE, CommonKeys.LABEL)\n",
    "image_only = None #TODO CommonKeys.IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans = Compose(\n",
    "    [\n",
    "        ScaleIntensityd(image_only),\n",
    "        AddChanneld(both_keys),\n",
    "        RandRotate90d(keys=both_keys, prob=aug_prob),\n",
    "        RandFlipd(keys=both_keys, prob=aug_prob),\n",
    "        RandZoomd(keys=both_keys, prob=aug_prob, mode=zoom_mode),\n",
    "        Rand2DElasticd(keys=both_keys, prob=aug_prob, spacing=10, magnitude_range=(-2, 2), mode=elast_mode),\n",
    "#         RandAffined(keys=both_keys, prob=aug_prob, rotate_range=1, translate_range=16, mode=elast_mode),\n",
    "        CastToTyped(both_keys, (np.float32, np.int32)),\n",
    "        EnsureTyped(both_keys),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_trans = Compose(\n",
    "    [\n",
    "        ScaleIntensityd(image_only),\n",
    "        AddChanneld(both_keys),\n",
    "        CastToTyped(both_keys, (np.float32, np.int32)),\n",
    "        EnsureTyped(both_keys),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a MONAI data loading object to compose batches during training, and another for validation: However, we need to replace `ArrayDataset` `ArrayDataset(train_images, image_trans, train_segs, seg_trans)` with `NPZDictItemDataset`\n",
    "\n",
    "check MONAI document for [NPZDictItemDataset](https://docs.monai.io/en/stable/data.html?highlight=NPZDictItemDataset#npzdictitemdataset)\n",
    "\n",
    "class monai.data.<b>NPZDictItemDataset</b>(`npzfile, keys, transform=None, other_keys=()`)\n",
    "\n",
    "Represents a dataset from a loaded NPZ file. The members of the file to load are named in the keys of keys and stored under the keyed name. All loaded arrays must have the same 0-dimension (batch) size. Items are always dicts mapping names to an item extracted from the loaded arrays. If passing slicing indices, will return a PyTorch Subset, for example: data: Subset = dataset[1:4], for more details, please check: https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "\n",
    "Parameters\n",
    "- <b>npzfile</b> (`Union[str, IO]`) â€“ Path to .npz file or stream containing .npz file data\n",
    "- <b>keys</b> (`Dict[str, str]`) â€“ Maps keys to load from file to name to store in dataset\n",
    "- <b>transform</b> (`Optional[Callable[â€¦, Dict[str, Any]]]`) â€“ Transform to apply to batch dict\n",
    "- <b>other_keys</b> (`Optional[Sequence[str]]`) â€“ secondary data to load from file and store in dict other_keys, not returned by __getitem__\n",
    "- <b>data</b> â€“ input data to load and transform to generate dataset for model.\n",
    "- <b>transform</b> â€“ a callable data transform on input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_trans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1780028/3925024728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mkeys_val\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m{\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCommonKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"segs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCommonKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNPZDictItemDataset\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnpz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_trans\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"caseIndices\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m#TODO #ArrayDataset  to NPZDictItemDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mval_dat\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mNPZDictItemDataset\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnpz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_val\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_trans\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;31m#TODO  #ArrayDataset  to NPZDictItemDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_trans' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# for keys \n",
    "from monai.utils.enums import CommonKeys\n",
    "\n",
    "# create training and validation datasets from the whole set of images, these will be resized below based on case indices\n",
    "\n",
    "keys_train = {\"images\": CommonKeys.IMAGE, \"segs\": CommonKeys.LABEL} #TODO\n",
    "keys_val =  {\"images\": CommonKeys.IMAGE, \"segs\": CommonKeys.LABEL} # TODO\n",
    "\n",
    "train_dat = NPZDictItemDataset( npz, keys_train , train_trans  , (\"caseIndices\",) ) #TODO #ArrayDataset  to NPZDictItemDataset\n",
    "val_dat   = NPZDictItemDataset( npz, keys_val , val_trans  ) #TODO  #ArrayDataset  to NPZDictItemDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure  train_loader and val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# extract the case indices array\n",
    "case_indices = train_dat.other_keys[\"caseIndices\"] # TODO data[\"caseIndices\"]   to train_dat\n",
    "val_index = case_indices[-6, 0]  # keep the last 6 cases for testing\n",
    "\n",
    "\n",
    "train_dat = train_dat[:val_index]\n",
    "val_dat = val_dat[val_index:]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dat,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dat,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 52\n",
      "8 2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dat), len(val_dat))\n",
    "print(len(train_loader), len(val_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define out simple network. This doesn't do a good job so consider how to improve it by adding layers or other elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monai Network\n",
    "MONAI provides predefined networks. we can easily import it. \n",
    "\n",
    "- [Layers](https://docs.monai.io/en/stable/networks.html#layers) : Act, Conv, Norm, Dropout, Flatten, Reshape, Pad, Pool, SkipConnection\n",
    "- [Blocks](https://docs.monai.io/en/stable/networks.html#module-monai.networks.blocks) : ADN, Convolution, Synamic UnetBlock, FCN, GCN, Squeeze-andExcitation, ResNeXt, SABlock, Transformer Block, \n",
    "- [Nets](https://docs.monai.io/en/stable/networks.html#nets)  : DenseNet121, EfficientNet, SegResNet, ResNet, SENet154, DyUNet, UNet, AutoEncoder, VarAutoEncoder, ViT, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define out simple network. This doesn't do a good job so consider how to improve it by adding layers or other elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLine Model \n",
    "\n",
    "SegNet : \n",
    " - Input \n",
    " - Conv2D\n",
    " - MaxPool2D\n",
    " - Conv3D\n",
    " - ConvT2D\n",
    " - Conv2d\n",
    " - Output \n",
    "\n",
    "<p><img align='left' src=\"https://miro.medium.com/max/1400/1*nGFy96r63GwSE_EsJDLMDw.png\" width=600>  </p>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # layer 1: convolution, normalization, downsampling\n",
    "            nn.Conv2d(1, 2, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1),\n",
    "            # layer 2\n",
    "            nn.Conv2d(2, 4, 3, 1, 1),\n",
    "            # layer 3\n",
    "            nn.ConvTranspose2d(4, 2, 3, 2, 1, 1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            # layer 4: output\n",
    "            nn.Conv2d(2, 1, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "net = SegNet()\n",
    "net = net.to(device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ Challenge: Improve Results and Implementation ðŸ‘ˆ\n",
    "\n",
    "\n",
    "### 2. Improve/Replace Network\n",
    "\n",
    "As you can see we're not getting good results from our network. The training loss values are jumping around and not decreasing much anymore. The validation score has topped out at 0.25, which is really poor. \n",
    "\n",
    "It's now up to you to improve the results of our segmentation task. The things to consider changing include the network itself, how data is loaded, how batches might be composed, and what transforms we want to use from MONAI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model \n",
    "\n",
    "### UNet\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "U Shaped Network was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. \n",
    "It is Fully Convolutional Network Model for the segmentation task with two paths(encoder and decoder) with 1x1 convolution skip connection similar as residual. \n",
    "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\" width=800>\n",
    "\n",
    "#### UNet in monai.networks\n",
    "class monai.networks.nets.<b>UNet </b> (`spatial_dims, in_channels, out_channels, channels, strides, kernel_size=3, up_kernel_size=3, num_res_units=0, act='PRELU', norm='INSTANCE', dropout=0.0, bias=True, dimensions=None` )\n",
    "Enhanced version of <br>UNet</b> which has residual units implemented with the `ResidualUnit` class. The residual part uses a convolution to change the input dimensions to match the output dimensions if this is necessary but will use `nn.Identity` if not. Refer to: [Link](https://link.springer.com/chapter/10.1007/978-3-030-12029-0_40).\n",
    "\n",
    "Each layer of the network has a encode and decode path with a skip connection between them. Data in the encode path is downsampled using strided convolutions (if strides is given values greater than 1) and in the decode path upsampled using strided transpose convolutions. These down or up sampling operations occur at the beginning of each block rather than afterwards as is typical in <b>UNet</b> implementations.\n",
    "\n",
    "To further explain this consider the first example network given below. This network has 3 layers with strides of 2 for each of the middle layers (the last layer is the bottom connection which does not down/up sample). Input data to this network is immediately reduced in the spatial dimensions by a factor of 2 by the first convolution of the residual unit defining the first layer of the encode part. The last layer of the decode part will upsample its input (data from the previous layer concatenated with data from the skip connection) in the first convolution. this ensures the final output of the network has the same shape as the input.\n",
    "\n",
    "Padding values for the convolutions are chosen to ensure output sizes are even divisors/multiples of the input sizes if the strides value for a layer is a factor of the input sizes. A typical case is to use strides values of 2 and inputs that are multiples of powers of 2. An input can thus be downsampled evenly however many times its dimensions can be divided by 2, so for the example network inputs would have to have dimensions that are multiples of 4. In the second example network given below the input to the bottom layer will have shape `(1, 64, 15, 15)` for an input of shape `(1, 1, 240, 240)` demonstrating the input being reduced in size spatially by 2**4.\n",
    "\n",
    "##### Parameters\n",
    " - <b>spatial_dims </b>(`int`)  â€“ number of spatial dimensions.\n",
    " - <b>in_channels </b>(`int`) â€“ number of input channels.\n",
    " - <b>out_channels </b>(`int`) â€“ number of output channels.\n",
    " - <b>channels</b> (`Sequence[int]`) â€“ sequence of channels. Top block first. The length of channels should be no less than 2.\n",
    " - <b>strides</b> (`Sequence[int]`) â€“ sequence of convolution strides. The length of stride should equal to len(channels) - 1.\n",
    " - <b>kernel_size</b> (`Union[Sequence[int], int]`) â€“ convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>up_kernel_size</b> (`Union[Sequence[int], int]`) â€“ upsampling convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>num_res_units</b>  (`int`)â€“ number of residual units. Defaults to 0.\n",
    " - <b>act</b> (`Union[Tuple, str]`) â€“ activation type and arguments. Defaults to PReLU.\n",
    " - <b>norm</b> (`Union[Tuple, str]`) â€“ feature normalization type and arguments. Defaults to instance norm.\n",
    " - <b>dropout</b> (`float`) â€“ dropout ratio. Defaults to no dropout.\n",
    " - <b>bias (`bool`)</b> â€“ whether to have a bias term in convolution blocks. Defaults to True. According to Performance Tuning Guide, if a conv layer is directly followed by a batch norm layer, bias should be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import AutoEncoder\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "net = AutoEncoder( spatial_dims =2, in_channels=1, out_channels=1, channels=(4, 8, 16, 32),  strides=(2, 2, 2, 2), ) \n",
    "#net = AutoEncoder( spatial_dims =2, in_channels=1, out_channels=1, channels=(8, 16, 32, 64), strides=(2, 2, 2, 2), )\n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[16, 32, 64], strides=[2, 2], num_res_units=2, dropout=0.2) # 3 layers \n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[4, 8, 16], strides=[2, 2], num_res_units=2, dropout=0.2) # 3 layers \n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[4, 8, 16, 32,64], strides=[2,2, 2,2], num_res_units=3, dropout=0.4) # 5 layers\n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[16, 32, 64, 128, 256], strides=[2, 2, 2, 2], num_res_units=4, dropout=0.2) # 5 layers\n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[4, 8, 16, 32,64, 128,256], strides=[2, 2, 2, 2,2,2], num_res_units=6, dropout=0.2) # 7 layers\n",
    "\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "loss = DiceLoss(sigmoid=True)\n",
    "metric = DiceMetric(include_background=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "opt = torch.optim.Adam(net.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train loop [Baseline]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "step_losses = []\n",
    "epoch_metrics = []\n",
    "total_step = 0\n",
    "\n",
    "print(\"start train\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "\n",
    "    # train network with training images\n",
    "    for bimages, bsegs in loader:\n",
    "        bimages = bimages.to(device)\n",
    "        bsegs = bsegs.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        prediction = net(bimages)\n",
    "        loss_val = loss(torch.sigmoid(prediction), bsegs)\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        step_losses.append((total_step, loss_val.item()))\n",
    "        total_step += 1\n",
    "\n",
    "    net.eval()\n",
    "    metric_vals = []\n",
    "\n",
    "    # test our network using the validation dataset\n",
    "    with torch.no_grad():\n",
    "        for bimages, bsegs in val_loader:\n",
    "            bimages = bimages.to(device)\n",
    "            bsegs = bsegs.to(device)\n",
    "\n",
    "            #prediction = net(bimages)\n",
    "\n",
    "            mvals = metric(y_pred=torch.sigmoid(prediction) > 0.5, y=bsegs)\n",
    "            metric_vals += mvals.cpu().data.numpy().flatten().tolist()\n",
    "\n",
    "    epoch_metrics.append((total_step, np.average(metric_vals)))\n",
    "\n",
    "    progress_bar(epoch + 1, num_epochs, f\"Validation Metric: {epoch_metrics[-1][1]:7.3}\")\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now graph the results from our training and find the results are not very good:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ Challenge: Improve Results and Implementation ðŸ‘ˆ\n",
    "\n",
    "### 3. Replace The Training Loop\n",
    "\n",
    "This notebook uses a simple training loop with validation done explicitly. Replace this with a use of the `SupervisedTrainer` class and `SupervisedEvaluator` to do the evaluation throughout the training process. The graph plotting is done simply by recording values at each iteration through the loop, you'll want to use some other mechanism to do the same thing such as using a `MetricLogger` handler object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ignite\n",
    "in the end-to-end pipeline notebook,  we already use   `SupervisedTrainer` and `SupervisedEvaluator`. \n",
    "use MedNIST pipeline 03 and 04 for reference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_losses = []\n",
    "epoch_metrics = []\n",
    "metric_values = []\n",
    "iter_losses=[]\n",
    "batch_sizes=[]\n",
    "epoch_loss_values =[]\n",
    "total_step = 0\n",
    "max_epochs = 200\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, Activationsd, AsDiscreted\n",
    "\n",
    "post_transform = Compose(\n",
    "    [Activationsd(keys=\"pred\", sigmoid=True), AsDiscreted(keys=[\"pred\", \"label\"], threshold_values=True,),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.handlers import StatsHandler, MeanDice, from_engine\n",
    "from monai.engines import SupervisedEvaluator\n",
    "\n",
    "\n",
    "evaluator = SupervisedEvaluator(\n",
    "    device= device, #TODO\n",
    "    val_data_loader= val_loader, #TODO\n",
    "    network=net , #TODO\n",
    "    postprocessing= post_transform, #TODO\n",
    "    key_val_metric={\"val_mean_dice\": MeanDice(include_background=True, output_transform=from_engine([\"pred\", \"label\"]))},\n",
    "    val_handlers=[StatsHandler(output_transform=lambda x: None)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 50 8\n"
     ]
    }
   ],
   "source": [
    "from monai.handlers import MetricLogger, ValidationHandler\n",
    "from monai.engines import SupervisedTrainer\n",
    "\n",
    "logger = MetricLogger(evaluator=evaluator)\n",
    "\n",
    "trainer = SupervisedTrainer(\n",
    "    device= device, #TODO\n",
    "    max_epochs= max_epochs,  #TODO\n",
    "    train_data_loader= train_loader,  #TODO\n",
    "    network= net,  #TODO\n",
    "    optimizer= opt,  #TODO\n",
    "    loss_function= loss,  #TODO\n",
    "    train_handlers=[logger, ValidationHandler(1, evaluator)],\n",
    ")\n",
    "\n",
    "steps_per_epoch = len(train_dat) // train_loader.batch_size\n",
    "if len(train_dat) % train_loader.batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "print(len(train_dat) , train_loader.batch_size, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def _end_iter(engine: Engine):\n",
    "    global step\n",
    "    loss = np.average([o[\"loss\"] for o in engine.state.output])\n",
    "    batch_len = len(engine.state.batch[0])\n",
    "    epoch = engine.state.epoch\n",
    "    epoch_len = engine.state.max_epochs\n",
    "    step_total = engine.state.iteration  \n",
    "    iter_losses.append(loss)\n",
    "    batch_sizes.append(batch_len)\n",
    "\n",
    "    print(f\"\\nepoch {epoch}/{epoch_len}, step {step}/{steps_per_epoch},  total step {step_total}/{steps_per_epoch*epoch_len}, training_loss = {loss:.4f}\", end='')\n",
    "    step += 1\n",
    "    \n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def _end_epoch(engine: Engine):\n",
    "    global step\n",
    "    overall_average_loss = np.average(iter_losses, weights=batch_sizes)\n",
    "    epoch_loss_values.append(overall_average_loss)    \n",
    "    # clear the contents of iter_losses and batch_sizes for the next epoch\n",
    "    del iter_losses[:]\n",
    "    del batch_sizes[:]\n",
    "    \n",
    "    dice = evaluator.state.metrics[\"val_mean_dice\"]   \n",
    "    metric_values.append(dice)\n",
    "    #progress_bar(engine.state.epoch, num_epochs, f\"Validation Metric: {dice:7.3}\")\n",
    "    print(f\" | avg loss: {overall_average_loss:.4f} Dice Metric: {dice:7.3}\", end='')\n",
    "    step = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    " \n",
    "ax[0].plot(epoch_loss_values)\n",
    "ax[0].set_title(\"epoch Loss\")\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].plot(metric_values)\n",
    "ax[1].set_title(\"epoch matric\")\n",
    "ax[1].set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repeat trials \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "- [01_getting started](./01_getting.ipynb)\n",
    "\n",
    "- [02_pipeline_01](./02_pipeline_01.ipynb)\n",
    "- [02_pipeline_02 ](./02_pipeline_02.ipynb)\n",
    "- [02_pipeline_03](./02_pipeline_03.ipynb)\n",
    "- [02_pipeline_04 Next ](./02_pipeline_04.ipynb)\n",
    "\n",
    "- [03_brain_gan ](./03_brain_gan_01.ipynb)\n",
    "\n",
    "- [04_spleen_segment](./04_spleen_segment.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac baseline](./05_challenge_cardiac_baseline.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac workspace](./05_challenge_cardiac_workspace.ipynb) \n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "moani_bootcamp_2_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
